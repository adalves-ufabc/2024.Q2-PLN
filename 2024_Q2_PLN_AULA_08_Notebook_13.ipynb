{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalves-ufabc/2024.Q2-PLN/blob/main/2024_Q2_PLN_AULA_08_Notebook_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz8ST67pSdeF"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2024-Q2]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDFqzPZFSfBx"
      },
      "source": [
        "## **Introdução à API da OpenAI**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFIowMuI69Lv",
        "outputId": "eccbee21-9524-42ba-e34a-8dfb9040a496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12 (main, Mar 22 2024, 16:50:05) [GCC 11.4.0]\n"
          ]
        }
      ],
      "source": [
        "#@title Versão do Python no Google Colab\n",
        "\n",
        "import sys\n",
        "\n",
        "print(sys.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwdwRDIwC5Me"
      },
      "source": [
        "## **Configuração da API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjLmc6Yr1a2B",
        "outputId": "1cf480ce-31e2-4f01-9c74-0cd795837c85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.37.0-py3-none-any.whl (337 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.37.0\n"
          ]
        }
      ],
      "source": [
        "#@title Instalando a biblioteca da API da OpenAI\n",
        "\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbFq4KS1fa4T",
        "outputId": "b0c0f2e1-2d05-4597-a81a-4ea2aabfce9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.37.0\n"
          ]
        }
      ],
      "source": [
        "#@title Versão da API da OpenAI\n",
        "\n",
        "import openai\n",
        "\n",
        "print(openai.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Definindo a chave da API\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "OPENAI_API_KEY = getpass()\n",
        "\n",
        "#import os\n",
        "\n",
        "#os.environ[\"OPENAI_API_KEY\"] = getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InAV3BG2iF5z",
        "outputId": "2b7ae153-b6f4-4c5d-afa2-2cc4cb305f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O módulo `getpass` em Python é utilizado para obter senhas e outros tipos de entrada sensível do usuário sem que essas informações sejam exibidas na tela. Isso é útil para aumentar a segurança, evitando que a senha ou a chave da API sejam vistas por pessoas próximas ao usuário."
      ],
      "metadata": {
        "id": "V0YkYyCdif7j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz8DO2GyJdge"
      },
      "outputs": [],
      "source": [
        "#@title Primeiro teste\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "cliente = OpenAI(api_key= OPENAI_API_KEY)\n",
        "\n",
        "resposta = cliente.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\", \"content\": \"Diga que isso é um teste\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo-0125\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF5XYMZX9MQA",
        "outputId": "6fc6b7b6-9481-4ba0-8dc8-d7b99f0eba09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-9o8Ki2YeTJzAPgAbjQwZUxWysJugZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Isso é um teste.', role='assistant', function_call=None, tool_calls=None))], created=1721735460, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=6, prompt_tokens=14, total_tokens=20))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta.json()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "9SXubIDL9Q_L",
        "outputId": "6e5d8fe4-8208-44b6-b48c-a2bb1cc45529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"id\":\"chatcmpl-9o8Ki2YeTJzAPgAbjQwZUxWysJugZ\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"Isso é um teste.\",\"role\":\"assistant\",\"function_call\":null,\"tool_calls\":null}}],\"created\":1721735460,\"model\":\"gpt-3.5-turbo-0125\",\"object\":\"chat.completion\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":6,\"prompt_tokens\":14,\"total_tokens\":20}}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Formatando a resposta\n",
        "\n",
        "import json\n",
        "\n",
        "# Carregar o JSON em um dicionário Python\n",
        "dados = json.loads(resposta.json())\n",
        "\n",
        "# Converter o dicionário em uma string JSON formatada com indentação\n",
        "json_formatado = json.dumps(dados, indent=4, ensure_ascii=False)\n",
        "\n",
        "# Dividir a string JSON formatada em linhas individuais\n",
        "linhas = json_formatado.splitlines()\n",
        "\n",
        "# Exibir cada linha separadamente\n",
        "for linha in linhas:\n",
        "    print(linha)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nPU9VuSA4Kc",
        "outputId": "aeca1dc5-1e0a-48e6-83e9-5d0f2c966e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"id\": \"chatcmpl-9o8Df9MrsVpsBiIgbKPRdLGxhH4vW\",\n",
            "    \"choices\": [\n",
            "        {\n",
            "            \"finish_reason\": \"stop\",\n",
            "            \"index\": 0,\n",
            "            \"logprobs\": null,\n",
            "            \"message\": {\n",
            "                \"content\": \"Isso é um teste.\",\n",
            "                \"role\": \"assistant\",\n",
            "                \"function_call\": null,\n",
            "                \"tool_calls\": null\n",
            "            }\n",
            "        }\n",
            "    ],\n",
            "    \"created\": 1721735023,\n",
            "    \"model\": \"gpt-3.5-turbo-0125\",\n",
            "    \"object\": \"chat.completion\",\n",
            "    \"service_tier\": null,\n",
            "    \"system_fingerprint\": null,\n",
            "    \"usage\": {\n",
            "        \"completion_tokens\": 6,\n",
            "        \"prompt_tokens\": 14,\n",
            "        \"total_tokens\": 20\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(resposta.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQeIbxw2Cqj0",
        "outputId": "5a59c15e-c097-4c94-9a59-7283bbdf1200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Luiz Inácio Lula da Silva\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Lista de Modelos\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "cliente = OpenAI( api_key = OPENAI_API_KEY )\n",
        "\n",
        "# obter a lista de modelos\n",
        "modelos = cliente.models.list()"
      ],
      "metadata": {
        "id": "xjBBIDFowqFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imprimir total de modelos\n",
        "print(len(modelos.data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ho-tNOdQxCpQ",
        "outputId": "6ca3ea43-058a-401d-c8de-9edd7b56a2fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imprimir os nomes dos modelos\n",
        "for modelo in modelos.data:\n",
        "   print(modelo.id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LwybwgoxWmK",
        "outputId": "4dbaa4ca-33d4-4d79-fe0d-c573861c54f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dall-e-3\n",
            "whisper-1\n",
            "gpt-4-turbo-2024-04-09\n",
            "gpt-4-1106-preview\n",
            "dall-e-2\n",
            "gpt-4-turbo\n",
            "tts-1-hd-1106\n",
            "tts-1-hd\n",
            "gpt-4-turbo-preview\n",
            "gpt-4o-mini\n",
            "tts-1\n",
            "babbage-002\n",
            "gpt-4-0125-preview\n",
            "gpt-4o-2024-05-13\n",
            "text-embedding-3-small\n",
            "text-embedding-3-large\n",
            "tts-1-1106\n",
            "gpt-3.5-turbo\n",
            "gpt-3.5-turbo-instruct\n",
            "gpt-3.5-turbo-instruct-0914\n",
            "text-embedding-ada-002\n",
            "gpt-3.5-turbo-16k\n",
            "davinci-002\n",
            "gpt-3.5-turbo-0125\n",
            "gpt-3.5-turbo-1106\n",
            "gpt-4-0613\n",
            "gpt-4o-mini-2024-07-18\n",
            "gpt-4o\n",
            "gpt-4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Função para verificar se um modelo existe\n",
        "\n",
        "def verificar_modelo(nome_modelo):\n",
        "   modelos = cliente.models.list()\n",
        "   for modelo in modelos.data:\n",
        "      if modelo.id == nome_modelo:\n",
        "         return True\n",
        "   return False"
      ],
      "metadata": {
        "id": "q2qeGjE5xebY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(verificar_modelo(\"gpt-3.5-turbo\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61fQ69lrxqXo",
        "outputId": "462002ce-59eb-4aa5-c433-87cbcd3a8dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Função para verificar se parte do nome está na lista de modelos\n",
        "\n",
        "def verificar_modelos_parcial(parte_nome):\n",
        "   modelos = cliente.models.list()\n",
        "   nomes_modelos = [modelo.id for modelo in modelos.data]\n",
        "   modelos_identificados = [modelo for modelo in nomes_modelos if parte_nome.lower() in modelo.lower()]\n",
        "\n",
        "   return modelos_identificados"
      ],
      "metadata": {
        "id": "wVxNyS6BxtHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verificar_modelos_parcial(\"gpt\")"
      ],
      "metadata": {
        "id": "A9E1OrbLx5zC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5baa9300-d0ef-4e81-c0c2-efd60a55c4d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gpt-4-turbo-2024-04-09',\n",
              " 'gpt-4-1106-preview',\n",
              " 'gpt-4-turbo',\n",
              " 'gpt-4-turbo-preview',\n",
              " 'gpt-4o-mini',\n",
              " 'gpt-4-0125-preview',\n",
              " 'gpt-4o-2024-05-13',\n",
              " 'gpt-3.5-turbo',\n",
              " 'gpt-3.5-turbo-instruct',\n",
              " 'gpt-3.5-turbo-instruct-0914',\n",
              " 'gpt-3.5-turbo-16k',\n",
              " 'gpt-3.5-turbo-0125',\n",
              " 'gpt-3.5-turbo-1106',\n",
              " 'gpt-4-0613',\n",
              " 'gpt-4o-mini-2024-07-18',\n",
              " 'gpt-4o',\n",
              " 'gpt-4']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRZI4tjTM3o3"
      },
      "source": [
        "**Requisições HTTP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc9HteJ7MdB5",
        "outputId": "8c17b924-246c-4312-9413-5e40d27abbc4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-9o8cGdEWKmHtWWuqmjJKieWmOKl4v',\n",
              " 'object': 'chat.completion',\n",
              " 'created': 1721736548,\n",
              " 'model': 'gpt-3.5-turbo-0125',\n",
              " 'choices': [{'index': 0,\n",
              "   'message': {'role': 'assistant', 'content': 'Negativo'},\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop'}],\n",
              " 'usage': {'prompt_tokens': 70, 'completion_tokens': 2, 'total_tokens': 72},\n",
              " 'system_fingerprint': None}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "#@title Endpoint *Chat Completions* (Análise de Sentimentos)\n",
        "\n",
        "import requests\n",
        "\n",
        "endpoint = \"https://api.openai.com/v1/chat/completions\"\n",
        "\n",
        "mensagem_sistema = 'Você é um assistente que analisa sentimentos de avaliações de produtos'\n",
        "mensagem_usuario = \"Aqui está uma avaliação de um produto: 'Este produto é incrível!\"\n",
        "mensagem_assistente = \"Classifique o sentimento retornando apenas 'Positivo' ou 'Negativo'. \"\n",
        "\n",
        "parametros = {\n",
        "   \"model\": \"gpt-3.5-turbo-0125\",\n",
        "   \"messages\": [\n",
        "      {\"role\": \"system\", \"content\": mensagem_sistema},\n",
        "      {\"role\": \"user\", \"content\": mensagem_usuario},\n",
        "      {\"role\": \"assistant\", \"content\": mensagem_assistente}]\n",
        "}\n",
        "headers = {\n",
        "   \"Content-Type\": \"application/json\",\n",
        "   \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n",
        "}\n",
        "\n",
        "resposta = requests.post(endpoint, json=parametros, headers=headers)\n",
        "resposta.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x18QiMbUZbI4",
        "outputId": "65555a8c-bd84-4482-b14a-d9b52bebb982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positivo\n"
          ]
        }
      ],
      "source": [
        "# imprimir apenas resposta do assistente\n",
        "print(resposta.json()[\"choices\"][0][\"message\"][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCAM03rvnW1B"
      },
      "source": [
        "**Utilizando a biblioteca da *API***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title *Chat Completions*\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "cliente = OpenAI( api_key = OPENAI_API_KEY )\n",
        "\n",
        "resposta = cliente.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"user\", \"content\": \"Qual a capital do Brasil? Retorne apenas o nome\"}\n",
        "  ]\n",
        ")"
      ],
      "metadata": {
        "id": "xke_3x5XF6GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(resposta.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkuSw_OpHNhE",
        "outputId": "68860c1b-16a2-4269-e69c-94099ac8790f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brasília\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMXjHddkfiEW"
      },
      "outputs": [],
      "source": [
        "#@title *Chat Completions* (Correção Gramatical)\n",
        "\n",
        "mensagem_sistema = 'Você receberá instruções e sua tarefa é corrigir para o Português'\n",
        "mensagem_usuario = \"o mecado estava fexado.\"\n",
        "\n",
        "cliente = OpenAI( api_key = OPENAI_API_KEY )\n",
        "\n",
        "resposta = cliente.chat.completions.create(\n",
        "   model = \"gpt-3.5-turbo-0125\",\n",
        "   messages = [{\"role\": \"system\", \"content\": mensagem_sistema},\n",
        "               {\"role\": \"user\", \"content\": mensagem_usuario}]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJ4OKXgAf5rG",
        "outputId": "9fb10618-8c8c-4600-f8c8-fe1ba65faada"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-9o8mVnHWiiKFE3ivHB7difkuLMceY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='O mercado estava fechado.', role='assistant', function_call=None, tool_calls=None))], created=1721737183, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=7, prompt_tokens=38, total_tokens=45))"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "resposta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(resposta.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1D6itRZI5oa",
        "outputId": "6c029b1e-c0ba-4960-d67f-ddc80f1f5e03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O mercado estava fechado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jci5ArpagGG7",
        "outputId": "ea6260d9-4ff2-4485-8601-6a30aabf4828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chatcmpl-9o8mVnHWiiKFE3ivHB7difkuLMceY\n"
          ]
        }
      ],
      "source": [
        "# id\n",
        "print(resposta.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8bP8g1xgGG-",
        "outputId": "8b32fab3-321e-4fe6-eaf4-44d4a23874d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt-3.5-turbo-0125\n"
          ]
        }
      ],
      "source": [
        "# modelo\n",
        "print(resposta.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubcuMEWxgGG_",
        "outputId": "e57e8fc1-8809-49b8-fe3c-69917bcaf65c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45\n"
          ]
        }
      ],
      "source": [
        "# total de tokens\n",
        "print(resposta.usage.total_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Referências**:\n",
        "\n",
        "https://github.com/openai/openai-python\n",
        "\n",
        "https://platform.openai.com/docs/api-reference"
      ],
      "metadata": {
        "id": "nKBKOZ-QDNG-"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMi+nVQ3xUxJOu/0RGp038N",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}