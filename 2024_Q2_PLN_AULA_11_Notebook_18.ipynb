{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalves-ufabc/2024.Q2-PLN/blob/main/2024_Q2_PLN_AULA_11_Notebook_18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2024-Q2]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmK05FgcOzL2"
      },
      "source": [
        "## **LangChain**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYKEbnlNTVlR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1515740-71bc-4952-8e97-2a9c8932c262"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.6/377.6 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#@title Instalando o pacote LangChain\n",
        "!pip install langchain -q U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJvs5RTcbE64",
        "outputId": "ec4ee271-588b-4ec1-a952-3504e4239752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2.11\n"
          ]
        }
      ],
      "source": [
        "#@title Versão do LangChain\n",
        "\n",
        "import langchain\n",
        "\n",
        "print(langchain.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Integração com o pacote da OpenAI\n",
        "\n",
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "8klfbjqKbUpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM5RnKClbPtQ",
        "outputId": "9281c6b6-891c-4025-9f34-da62bd2ef2b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "#@title Definindo a chave da API da OpenAI\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5GGZ0e66kma"
      },
      "source": [
        "## **Cadeia (*Chain*)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No **LangChain**, uma **cadeia** é uma sequência de operações que conecta vários componentes, como modelos de linguagem, *parsers* de saída (um componente que processa a saída do modelo para formatação) e transformações de dados, para criar um fluxo de trabalho coeso. A **cadeia** permite combinar diferentes passos de processamento e manipulação de dados de maneira estruturada e eficiente.\n",
        "\n",
        "As **cadeias** são configuradas para permitir um fluxo contínuo de dados entre os componentes, facilitando a construção de aplicações complexas de PLN."
      ],
      "metadata": {
        "id": "P00KwFs3EtUn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ygo9oZC8qgV"
      },
      "source": [
        "Podemos então inicializar o modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QUTKqod4en0"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "modelo = ChatOpenAI( temperature = 0.9, max_tokens= 50 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zflVvVO_83Nj"
      },
      "source": [
        "Depois de instalar e inicializar o modelo de sua escolha, podemos tentar usá-lo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8CL95XV4whj",
        "outputId": "859e6f6b-4bb6-4163-9121-5d961900c7f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='ColorMeias', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 38, 'total_tokens': 41}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4daeb7f0-04ea-40d1-ba46-a3465444c341-0', usage_metadata={'input_tokens': 38, 'output_tokens': 3, 'total_tokens': 41})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "prompt = \"Qual seria um bom nome para uma empresa que fabrica meias coloridas? Responda em Português e retorne apenas o nome da empresa\"\n",
        "\n",
        "modelo.invoke(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyD8bN8kr33V"
      },
      "source": [
        "**Templates de *prompt***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO_pNKVp9sFt"
      },
      "source": [
        "Também podemos orientar a resposta com um template de *prompt*. Os templates de *prompt* são usados para converter a entrada bruta do usuário em uma entrada melhor para o modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`PromptTemplate`**"
      ],
      "metadata": {
        "id": "9wqgDjAkF2X5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6D83NtUG90Sx"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables = [\"produto\"],\n",
        "    template = \"Qual seria um bom nome para uma empresa que fabrica {produto}? Responda em Português e retorne apenas o nome da empresa?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwq9WW61-QJd"
      },
      "source": [
        "Agora podemos combiná-los em uma cadeia simples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uyqY35m-A4C"
      },
      "outputs": [],
      "source": [
        "chain = prompt | modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tre_BeCW-HPH",
        "outputId": "0ad9bc80-1249-4d67-8137-cde240983d5e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Arco-Íris Meias Coloridas.', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 39, 'total_tokens': 49}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-91b7c37b-9330-49d7-8069-3d27ade1fb4e-0', usage_metadata={'input_tokens': 39, 'output_tokens': 10, 'total_tokens': 49})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "chain.invoke({\"produto\": \"meias coloridas\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`ChatPromptTemplate`**"
      ],
      "metadata": {
        "id": "IN3LCIgeFvVb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgMoHyBhIWMN"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Você é um estrategista de marketing digital especializado em {setor}\"),\n",
        "    (\"user\", \"Considerando a crescente digitalização no {setor}, proponha um slogan cativante para uma campanha publicitária.\")\n",
        "])\n",
        "\n",
        "chain = prompt | modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Gzm3PzNJKbQ",
        "outputId": "6c41370b-d93f-48d1-c9fb-eb2abc62b374"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='\"Transforme sua contabilidade, conecte-se ao futuro!\"', response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 55, 'total_tokens': 68}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d9ae5d3d-9f66-425a-aa55-14f34f447c39-0', usage_metadata={'input_tokens': 55, 'output_tokens': 13, 'total_tokens': 68})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "chain.invoke({\"setor\": \"contábil\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`MessagesPlaceholder`**"
      ],
      "metadata": {
        "id": "gCtPz2LhHhs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este modelo de *prompt* é responsável por adicionar uma lista de mensagens em um determinado local.\n",
        "\n",
        " `MessagesPlaceholder` é uma classe usada no **LangChain** para representar um espaço reservado dentro de um modelo de *prompt*, onde mensagens dinâmicas podem ser inseridas posteriormente. Ele atua como um marcador que indica onde as mensagens devem ser incluídas na estrutura do *prompt*."
      ],
      "metadata": {
        "id": "PfpGORbdHkPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Define o template de prompt com uma mensagem do sistema e um espaço reservado para mensagens do usuário\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Você é um assistente de viagem experiente. Ajude o usuário a planejar uma viagem.\"),\n",
        "    MessagesPlaceholder(\"usuario_msgs\")\n",
        "])\n",
        "\n",
        "# Cria um dicionário com mensagens do usuário\n",
        "mensagens_usuario = [\n",
        "    HumanMessage(content=\"Estou planejando uma viagem para a Europa. Quais são as melhores cidades para visitar?\"),\n",
        "    HumanMessage(content=\"Pode me dar sugestões de atividades em Paris?\")\n",
        "]\n",
        "\n",
        "# Invoca o template com as mensagens do usuário\n",
        "resultado = prompt_template.invoke({\"usuario_msgs\": mensagens_usuario})"
      ],
      "metadata": {
        "id": "fwRSRfWCr2SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultado"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TziWWYSsmbw",
        "outputId": "ffcf4ebb-5fdd-475d-ff21-8f952917cb34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Você é um assistente de viagem experiente. Ajude o usuário a planejar uma viagem.'), HumanMessage(content='Estou planejando uma viagem para a Europa. Quais são as melhores cidades para visitar?'), HumanMessage(content='Pode me dar sugestões de atividades em Paris?')])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resultado.messages[0].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CPTj20Grsch7",
        "outputId": "c786821d-33ed-459a-b2bd-e90e435f0e33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Você é um assistente de viagem experiente. Ajude o usuário a planejar uma viagem.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrando com um modelo de linguagem:"
      ],
      "metadata": {
        "id": "4IFrzt7ZzOA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "modelo = ChatOpenAI( temperature = 0.9, max_tokens= 512 )"
      ],
      "metadata": {
        "id": "Hzh-YuKuzqMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template | modelo"
      ],
      "metadata": {
        "id": "wRLfxQvRyR3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = chain.invoke({\"usuario_msgs\": mensagens_usuario})"
      ],
      "metadata": {
        "id": "_MU1J192yZ4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(resposta.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "jScr9Bk6zfuY",
        "outputId": "5e201059-5729-4bbb-dd1a-22224da5aeb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Claro! Paris é uma cidade incrível com muitas opções de atividades. Aqui estão algumas sugestões do que fazer na Cidade Luz:\n\n1. Visite a Torre Eiffel: Suba até o topo para ter uma vista deslumbrante da cidade.\n2. Passeie pelo Louvre: Explore um dos maiores e mais famosos museus do mundo, lar de obras-primas como a Mona Lisa.\n3. Caminhe ao longo do rio Sena: Aproveite a beleza da cidade enquanto passeia pelas margens do rio.\n4. Conheça a Catedral de Notre-Dame: Visite essa icônica catedral gótica, que é um marco histórico de Paris.\n5. Faça um passeio de barco pelo Sena: Desfrute de uma vista panorâmica da cidade a partir da água.\n6. Visite o Palácio de Versalhes: Faça uma viagem de um dia para conhecer esse magnífico palácio e seus belos jardins.\n7. Explore o bairro de Montmartre: Descubra a atmosfera boêmia desse bairro, lar do famoso Moulin Rouge e da Basílica de Sacré-Cœur.\n8. Delicie-se com a gastronomia francesa: Experimente os deliciosos queijos, vinhos, croissants e outras iguarias locais.\n\nEssas são apenas algumas das muitas atividades que Paris tem a oferecer. Aproveite sua viagem!"
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Referência**:\n",
        "\n",
        "> https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UZzNfb7WeTaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LangChain Expression Language (LCEL)**"
      ],
      "metadata": {
        "id": "mekuFSiI6sfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A ***LangChain Expression Language*** é uma forma de criar cadeias personalizadas e arbitrárias. Ela é construída sobre o protocolo `Runnable`.\n",
        "\n",
        "O protocolo `Runnable` do **LangChain** é uma interface que define como diferentes componentes podem ser encadeados e executados dentro de um fluxo de trabalho."
      ],
      "metadata": {
        "id": "dfNU7xsc6qPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Como encadear *runnables***"
      ],
      "metadata": {
        "id": "xuKCjEcU6_-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Um ponto sobre a **LCEL** é que qualquer dois *runnables* podem ser 'encadeados' juntos em sequências. A saída da chamada `.invoke()` do *runnable* anterior é passada como entrada para o próximo *runnable*. Isso pode ser feito usando o operador pipe (`|`), ou o método mais explícito `.pipe()`, que faz a mesma coisa.\n",
        "\n",
        "A *RunnableSequence* resultante é em si mesma um *runnable*, o que significa que pode ser invocada, transmitida ou encadeada ainda mais, assim como qualquer outro *runnable*."
      ],
      "metadata": {
        "id": "tU-1g2OR73th"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para demonstrar como isso funciona, vamos apresentar um exemplo:"
      ],
      "metadata": {
        "id": "FXPSz2M58QpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "modelo = ChatOpenAI(model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "CMQbYqFx8nRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"me conte uma piada sobre {assunto}\")\n",
        "\n",
        "chain = prompt | modelo | StrOutputParser()"
      ],
      "metadata": {
        "id": "S-8oXgR88yIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"assunto\": \"papagaio\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TMbh8lGc9HYb",
        "outputId": "86ee6d4d-decd-4050-f500-dd2ca22b7606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Por que o papagaio foi expulso da escola?\\n\\nPorque ele só sabia repetir de ano em ano!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Coerção**"
      ],
      "metadata": {
        "id": "45bYkgVD9mti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos até combinar essa cadeia com mais *runnables* para criar outra cadeia. Isso pode envolver algum formato de entrada/saída usando outros tipos de *runnables*, dependendo dos requisitos de entrada e saída dos componentes da cadeia.\n",
        "\n",
        "Por exemplo, digamos que queremos compor a cadeia de geração de piadas com outra cadeia que avalia se a piada gerada foi engraçada ou não."
      ],
      "metadata": {
        "id": "vcoKX4xi9obx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "prompt_analise = ChatPromptTemplate.from_template(\"essa piada é engraçada? {piada}\")\n",
        "\n",
        "chain_composto = {\"piada\": chain} | prompt_analise | modelo | StrOutputParser()\n",
        "\n",
        "chain_composto.invoke({\"assunto\": \"papagaio\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "UqxEG7U_9-MY",
        "outputId": "488e698a-3c9c-45e4-e201-469c0a307881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sim, essa piada é engraçada porque faz um trocadilho com o fato de que papagaios são conhecidos por repetir o que ouvem, e no caso do advogado, ele repetiria o que o cliente diz durante o processo.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No contexto do **LangChain**, **coerção** refere-se ao processo de converter automaticamente ou adaptar um tipo de dado para outro, de modo que ele se encaixe nos requisitos de entrada ou saída de uma cadeia ou componente específico. Isso pode acontecer, por exemplo, quando você passa dados entre diferentes *runnables* (componentes que seguem o protocolo `Runnable`) em uma cadeia, e o **LangChain** ajusta automaticamente os dados para que sejam compatíveis com o próximo componente.\n",
        "\n",
        "Por exemplo, se você passar um dicionário como entrada em uma cadeia que espera um formato específico, o **LangChain** pode coagir (converter) automaticamente esse dicionário para o formato adequado, como um `RunnableParallel`, que executa operações em paralelo.\n",
        "\n",
        "Esse mecanismo de coerção facilita a construção de cadeias complexas, pois minimiza a necessidade de manipulação manual dos dados para garantir que sejam compatíveis com cada etapa do processo."
      ],
      "metadata": {
        "id": "yREWWO16-k4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**O método `.pipe()`**"
      ],
      "metadata": {
        "id": "XGd0uDIF_cr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "chain_composto_pipe = (\n",
        "    RunnableParallel({\"piada\": chain})\n",
        "    .pipe(prompt_analise)\n",
        "    .pipe(modelo)\n",
        "    .pipe(StrOutputParser())\n",
        ")\n",
        "\n",
        "chain_composto_pipe.invoke({\"assunto\": \"papagaio\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "kMsbmik__iNs",
        "outputId": "5fb32d1c-201e-4b9a-8923-75b97aca7993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sim, essa piada é engraçada porque faz uma brincadeira com o fato de que os papagaios costumam repetir o que ouvem. Eles não gostam de piadas porque acham que são sempre as mesmas e previsíveis.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ou de forma abreviada:"
      ],
      "metadata": {
        "id": "_EnowCmEBbu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain_composto_pipe = RunnableParallel({\"piada\": chain}).pipe(\n",
        "    prompt_analise, modelo, StrOutputParser()\n",
        ")\n",
        "\n",
        "chain_composto_pipe.invoke({\"assunto\": \"papagaio\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "u2cOvxLaBWhT",
        "outputId": "ef06af0c-8cb7-4e23-85b9-d54b13a1f402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sim, essa piada é engraçada porque brinca com a ideia de um papagaio que fala demais e acaba sendo expulso da escola por isso. A situação inusitada e o trocadilho com o fato do papagaio falar \"besteira\" durante as aulas contribuem para o humor da piada.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Função de Depuração**\n",
        "\n",
        "Uma função de depuração é uma função usada para inspecionar, verificar ou diagnosticar o estado de um programa durante sua execução."
      ],
      "metadata": {
        "id": "jmF_bxQKA99l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "def funcao_debug(output):\n",
        "    print(\"Piada gerada:\", output[\"piada\"])\n",
        "    return output\n",
        "\n",
        "chain_composto_pipe = (\n",
        "    RunnableParallel({\"piada\": chain})\n",
        "    .pipe(funcao_debug)  # adiciona uma função de depuração\n",
        "    .pipe(prompt_analise)\n",
        "    .pipe(modelo)\n",
        "    .pipe(StrOutputParser())\n",
        ")\n",
        "\n",
        "chain_composto_pipe.invoke({\"assunto\": \"papagaio\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "H3VZzwS4Ag0K",
        "outputId": "56d405d7-2d06-43ab-99b2-51c1d58bdde5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Piada gerada: Por que o papagaio não consegue segurar um segredo? Porque ele sempre vai acabar papagaiando!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sim, é engraçada porque brinca com o fato de que os papagaios são conhecidos por repetir palavras e segredos, então é impossível para eles manterem algo em segredo.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paralelizar etapas**\n",
        "\n",
        "`RunnableParallels` facilitam a execução de múltiplos *runnables* em paralelo e o retorno da saída desses *runnables* como um mapa."
      ],
      "metadata": {
        "id": "M2eoWy8DF2Dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "modelo = ChatOpenAI()\n",
        "\n",
        "chain_piada = ChatPromptTemplate.from_template(\"me conte uma piada sobre {assunto}\") | modelo\n",
        "chain_poema = ChatPromptTemplate.from_template(\"escreva um poema de 2 linhas sobre {assunto}\") | modelo\n",
        "\n",
        "chain_mapa = RunnableParallel(piada = chain_piada, poema = chain_poema)\n",
        "\n",
        "resposta = chain_mapa.invoke({\"assunto\": \"papagaio\"})"
      ],
      "metadata": {
        "id": "k8ZMAbJZF_Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUb0eATRG3XB",
        "outputId": "ab9366f8-e595-4d19-c65b-9a2c86846628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'piada': AIMessage(content='Por que o papagaio não entra no barco? Porque ele tem medo de se afogar em tanto \"papagaio\"!', response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 16, 'total_tokens': 48}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e70af190-be80-49d9-9566-b0082d8f681a-0', usage_metadata={'input_tokens': 16, 'output_tokens': 32, 'total_tokens': 48}),\n",
              " 'poema': AIMessage(content='Papagaio colorido, voa no céu brilhante\\nCom seu canto alegre, encanta toda a gente.', response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 22, 'total_tokens': 53}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-34b0aff1-e80f-41b0-a6c1-5cbddc64025e-0', usage_metadata={'input_tokens': 22, 'output_tokens': 31, 'total_tokens': 53})}"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yx_9fk7G52N",
        "outputId": "6b36b3ec-ac81-4569-a037-bb965b5f6787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['piada', 'poema'])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta[\"piada\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re---zkfG873",
        "outputId": "09d1a009-d9ec-49d8-a3e5-0be54f6cdc9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Por que o papagaio não entra no barco? Porque ele tem medo de se afogar em tanto \"papagaio\"!', response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 16, 'total_tokens': 48}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e70af190-be80-49d9-9566-b0082d8f681a-0', usage_metadata={'input_tokens': 16, 'output_tokens': 32, 'total_tokens': 48})"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta[\"piada\"].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "exDMS4eGHesN",
        "outputId": "c6e39d47-2581-4ece-ddb3-0b07ae2e2fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Por que o papagaio não entra no barco? Porque ele tem medo de se afogar em tanto \"papagaio\"!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`RunnableParallel` também é útil para executar processos independentes em paralelo, uma vez que cada *Runnable* no mapa é executado em paralelo. Por exemplo, podemos ver que nossas cadeias anteriores `chain_piada`, `chain_poema` e `chain_mapa` têm tempos de execução semelhantes, mesmo que `chain_mapa` execute as duas outras cadeias."
      ],
      "metadata": {
        "id": "dTyvqKMGHtbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "\n",
        "chain_piada.invoke({\"assunto\": \"papagaio\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o48wUrRdH8JA",
        "outputId": "90c0dc3c-ad55-4761-b680-e443fd7e43e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "960 ms ± 74.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "\n",
        "chain_poema.invoke({\"assunto\": \"papagaio\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8uXRCn5IF7o",
        "outputId": "d3aa2e59-2d3b-48a8-ec17-fe28d2809d57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "888 ms ± 285 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "\n",
        "chain_mapa.invoke({\"assunto\": \"papagaio\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6PWXxaVINjo",
        "outputId": "7f8924d2-e9cd-4aaa-f246-1d42b6d393ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "996 ms ± 166 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3DlNC/Hyi9ucSbeeXaufL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}