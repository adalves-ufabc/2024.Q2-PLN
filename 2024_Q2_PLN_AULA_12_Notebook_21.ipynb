{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalves-ufabc/2024.Q2-PLN/blob/main/2024_Q2_PLN_AULA_12_Notebook_21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2024-Q2]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmK05FgcOzL2"
      },
      "source": [
        "## **API da Groq**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Groq** é uma empresa de tecnologia que desenvolve hardware e software voltados para computação de alto desempenho, particularmente em aplicações de IA e aprendizado de máquina. Fundada por ex-engenheiros da Google, a **Groq** é conhecida por seus chips de processamento (chamados de \"*Tensor Streaming Processor*\" ou *TSP*) que são projetados para acelerar tarefas de IA, como a inferência de modelos de aprendizado profundo."
      ],
      "metadata": {
        "id": "pGqVQ50gypCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para criar uma chave:\n",
        "\n",
        "> https://console.groq.com/keys"
      ],
      "metadata": {
        "id": "lnBY4ssQyaz1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYKEbnlNTVlR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b2443cd-764c-490a-a5f5-4eace9ca6ff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#@title Instalando a biblioteca da API\n",
        "!pip install -q -U groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJvs5RTcbE64",
        "outputId": "4be9d655-ff73-4dc1-e0da-e9da8cfb6464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9.0\n"
          ]
        }
      ],
      "source": [
        "#@title Versão da API\n",
        "\n",
        "import groq\n",
        "\n",
        "print(groq.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM5RnKClbPtQ",
        "outputId": "94178bd2-297b-44cd-a712-f3e3e8804590"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "#@title Definindo a chave da API\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Teste\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "cliente = Groq()\n",
        "\n",
        "resposta = cliente.chat.completions.create(\n",
        "    messages = [{\"role\": \"user\", \"content\": \"Qual a capital do Brasil? Responde apenas o nome\"}],\n",
        "    model = \"llama3-8b-8192\",\n",
        ")"
      ],
      "metadata": {
        "id": "QBU58sBzwcak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACMUndIiyO3b",
        "outputId": "b858018d-e446-439d-9e5b-f05728d3e323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-a6874be6-aead-4324-8664-73e226115edd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Brasília', role='assistant', function_call=None, tool_calls=None))], created=1722944826, model='llama3-8b-8192', object='chat.completion', system_fingerprint='fp_179b0f92c9', usage=CompletionUsage(completion_tokens=4, prompt_tokens=21, total_tokens=25, completion_time=0.002426555, prompt_time=0.003841207, queue_time=None, total_time=0.006267762), x_groq={'id': 'req_01j4kr7bdsf7xrdfh0dhztkg6g'})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(resposta.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnK6usTpyMTo",
        "outputId": "a271bf3a-2462-4b32-ddc5-fd07275e22dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brasília\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Lista de Modelos\n",
        "\n",
        "import requests\n",
        "import os\n",
        "\n",
        "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
        "\n",
        "url = \"https://api.groq.com/openai/v1/models\"\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {api_key}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "resposta = requests.get(url, headers=headers)"
      ],
      "metadata": {
        "id": "eC4VmnEPy-Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta.json()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FH-HOWrzReH",
        "outputId": "3c0df5be-a63f-48d9-ffac-358ad9a80ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'object': 'list',\n",
              " 'data': [{'id': 'gemma2-9b-it',\n",
              "   'object': 'model',\n",
              "   'created': 1693721698,\n",
              "   'owned_by': 'Google',\n",
              "   'active': True,\n",
              "   'context_window': 8192,\n",
              "   'public_apps': None},\n",
              "  {'id': 'gemma-7b-it',\n",
              "   'object': 'model',\n",
              "   'created': 1693721698,\n",
              "   'owned_by': 'Google',\n",
              "   'active': True,\n",
              "   'context_window': 8192,\n",
              "   'public_apps': None},\n",
              "  {'id': 'llama-3.1-70b-versatile',\n",
              "   'object': 'model',\n",
              "   'created': 1693721698,\n",
              "   'owned_by': 'Meta',\n",
              "   'active': True,\n",
              "   'context_window': 131072,\n",
              "   'public_apps': None},\n",
              "  {'id': 'llama-3.1-8b-instant',\n",
              "   'object': 'model',\n",
              "   'created': 1693721698,\n",
              "   'owned_by': 'Meta',\n",
              "   'active': True,\n",
              "   'context_window': 131072,\n",
              "   'public_apps': None},\n",
              "  {'id': 'llama3-70b-8192',\n",
              "   'object': 'model',\n",
              "   'created': 1693721698,\n",
              "   'owned_by': 'Meta',\n",
              "   'active': True,\n",
              "   'context_window': 8192,\n",
              "   'public_apps': None},\n",
              "  {'id': 'llama3-8b-8192',\n",
              "   'object': 'model',\n",
              "   'created': 1693721698,\n",
              "   'owned_by': 'Meta',\n",
              "   'active': True,\n",
              "   'context_window': 8192,\n",
              "   'public_apps': None},\n",
              "  {'id': 'llama3-groq-70b-8192-tool-use-preview',\n",
              "   'object': 'model',\n",
              "   'created': 1693721698,\n",
              "   'owned_by': 'Groq',\n",
              "   'active': True,\n",
              "   'context_window': 8192,\n",
              "   'public_apps': None},\n",
              "  {'id': 'llama3-groq-8b-8192-tool-use-preview',\n",
              "   'object': 'model',\n",
              "   'created': 1693721698,\n",
              "   'owned_by': 'Groq',\n",
              "   'active': True,\n",
              "   'context_window': 8192,\n",
              "   'public_apps': None},\n",
              "  {'id': 'llama-guard-3-8b',\n",
              "   'object': 'model',\n",
              "   'created': 1693721698,\n",
              "   'owned_by': 'Meta',\n",
              "   'active': True,\n",
              "   'context_window': 8192,\n",
              "   'public_apps': None},\n",
              "  {'id': 'mixtral-8x7b-32768',\n",
              "   'object': 'model',\n",
              "   'created': 1693721698,\n",
              "   'owned_by': 'Mistral AI',\n",
              "   'active': True,\n",
              "   'context_window': 32768,\n",
              "   'public_apps': None},\n",
              "  {'id': 'whisper-large-v3',\n",
              "   'object': 'model',\n",
              "   'created': 1693721698,\n",
              "   'owned_by': 'OpenAI',\n",
              "   'active': True,\n",
              "   'context_window': 1500,\n",
              "   'public_apps': None}]}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dados = resposta.json()\n",
        "\n",
        "largura_id = max(len(model['id']) for model in dados['data']) + 2\n",
        "largura_owner = max(len(model['owned_by']) for model in dados['data']) + 2\n",
        "\n",
        "print(f\"{'ID'.ljust(largura_id)} Proprietário\")\n",
        "\n",
        "for modelo in dados['data']:\n",
        "    print(f\"{modelo['id'].ljust(largura_id)} {modelo['owned_by']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4OTVNaN0XEo",
        "outputId": "282f01d4-d493-4b60-dbbf-4f9f752c9b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID                                      Proprietário\n",
            "gemma2-9b-it                            Google\n",
            "gemma-7b-it                             Google\n",
            "llama-3.1-70b-versatile                 Meta\n",
            "llama-3.1-8b-instant                    Meta\n",
            "llama3-70b-8192                         Meta\n",
            "llama3-8b-8192                          Meta\n",
            "llama3-groq-70b-8192-tool-use-preview   Groq\n",
            "llama3-groq-8b-8192-tool-use-preview    Groq\n",
            "llama-guard-3-8b                        Meta\n",
            "mixtral-8x7b-32768                      Mistral AI\n",
            "whisper-large-v3                        OpenAI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://console.groq.com/docs/quickstart\n",
        "\n",
        "https://github.com/groq/groq-api-cookbook"
      ],
      "metadata": {
        "id": "PhZvR1VSx7js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Integração com o LangChain**"
      ],
      "metadata": {
        "id": "H8uVt3ChbVtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3aTfMcFbqbi",
        "outputId": "d2086768-9f28-4536-d388-0b866daae98c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/379.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m378.9/379.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.9/379.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.2/140.2 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "modelo = ChatGroq(model=\"llama3-8b-8192\")"
      ],
      "metadata": {
        "id": "kqtjvemCbxeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "mensagens = [\n",
        "    SystemMessage(content=\"Traduza o seguinte do Inglês para o Português\"),\n",
        "    HumanMessage(content=\"hi!\"),\n",
        "]\n",
        "\n",
        "resposta = modelo.invoke(mensagens)"
      ],
      "metadata": {
        "id": "CMLBK8tob10m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTPrtwKBcJMm",
        "outputId": "b4c8cde2-b1c1-41f8-f6ce-b99671656746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Olá!', response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 32, 'total_tokens': 36, 'completion_time': 0.002400138, 'prompt_time': 0.005497658, 'queue_time': None, 'total_time': 0.007897796}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None}, id='run-9a0792b6-c468-4314-8c1b-dd265b3053c3-0', usage_metadata={'input_tokens': 32, 'output_tokens': 4, 'total_tokens': 36})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CGDDUr7zcLWy",
        "outputId": "1a7692bc-0a6b-47aa-92bc-129338048f87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Olá!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ChatPromptTemplate**"
      ],
      "metadata": {
        "id": "hXo-EcS2cQWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_template = \"Traduza o seguinte para {idioma}:\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_template), (\"user\", \"{texto}\")]\n",
        ")\n",
        "\n",
        "resultado = prompt_template.invoke({\"idioma\": \"italiano\", \"texto\": \"hi\"})\n",
        "\n",
        "resultado"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHBakft8f9Vl",
        "outputId": "1e9d432c-8d52-4991-bf3e-dd10b60c9fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Traduza o seguinte para italiano:'), HumanMessage(content='hi')])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resultado.to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqn0u8fMgVkB",
        "outputId": "29e3a9fe-793f-4851-cc0b-0f9e4f02f777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='Traduza o seguinte para italiano:'),\n",
              " HumanMessage(content='hi')]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain =  prompt_template | modelo\n",
        "\n",
        "resposta = chain.invoke({\"idioma\": \"português\", \"texto\": \"hi\"})"
      ],
      "metadata": {
        "id": "02pE5aUqXaGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSf0ZxSUkYoF",
        "outputId": "9ed46709-bd0f-4524-94ba-36310a65022d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Olá!', response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 27, 'total_tokens': 31, 'completion_time': 0.002408538, 'prompt_time': 0.005813214, 'queue_time': None, 'total_time': 0.008221751999999999}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None}, id='run-554ae75c-c63d-4744-9b7f-3f8d16805ca2-0', usage_metadata={'input_tokens': 27, 'output_tokens': 4, 'total_tokens': 31})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kpO8W3PIkaHa",
        "outputId": "cf422a6b-93f2-40cd-9941-440d0d581873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Olá!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encadeando componentes com LCEL**"
      ],
      "metadata": {
        "id": "cV0BuA6dgjHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "VjipoOCkghDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template | modelo | parser"
      ],
      "metadata": {
        "id": "DSvspJQbgbFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"idioma\": \"portugues\", \"texto\": \"hi\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3iJp6twogrPE",
        "outputId": "cf0985cc-8adf-49a8-d201-f2d7c40e9471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Olá!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cadeias**"
      ],
      "metadata": {
        "id": "UFDo_ErJhWIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.messages import HumanMessage"
      ],
      "metadata": {
        "id": "dQ8iJz0qjYqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = ChatGroq()"
      ],
      "metadata": {
        "id": "FSkaLZdVjaN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"Me conte uma piada sobre {assunto}\")\n",
        "chain = prompt | modelo"
      ],
      "metadata": {
        "id": "n6ruRLp3jIoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = chain.invoke({\"assunto\": \"papagaio\"})"
      ],
      "metadata": {
        "id": "ubjzAw-3jesT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(resposta.content)"
      ],
      "metadata": {
        "id": "dtN95fuclcvO",
        "outputId": "43384e22-8f15-4ec4-bc41-a47cb7bbb393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Claro, aqui está uma piada engraçada sobre papagaio:\n\nPor que o papagaio nunca fica sem dinheiro?\n\nPorque ele sempre diz: \"Dá-me mais um real, por favor!\"\n\nEspero que você tenha se divertido com essa piada sobre papagaio!"
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mais um exemplo:"
      ],
      "metadata": {
        "id": "vGmxLMnazSwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "modelo = ChatGroq(model=\"llama3-8b-8192\")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Você é um assistente que traduz {idioma_input} para {idioma_output}.\",\n",
        "        ),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | modelo\n",
        "\n",
        "resposta = chain.invoke(\n",
        "    {\n",
        "        \"idioma_input\": \"Inglês\",\n",
        "        \"idioma_output\": \"Português\",\n",
        "        \"input\": \"I love programming.\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "aW0B_raRnMhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kPrGvXoynWaj",
        "outputId": "fea0455a-1964-43e9-9e18-e1d5eea198a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Eu amo programação.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Referência**:\n",
        "\n",
        "> https://python.langchain.com/v0.2/docs/tutorials/llm_chain/"
      ],
      "metadata": {
        "id": "V5PcmDF3dL_w"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODn1MocW51dUZ1wgFl7psv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}