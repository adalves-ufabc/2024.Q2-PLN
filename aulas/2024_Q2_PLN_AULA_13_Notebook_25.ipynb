{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalves-ufabc/2024.Q2-PLN/blob/main/2024_Q2_PLN_AULA_13_Notebook_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2024-Q2]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmK05FgcOzL2"
      },
      "source": [
        "## **LangChain**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYKEbnlNTVlR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5ab4959-6a9b-4298-d162-3799a06d4fad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m950.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m990.6/990.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m379.9/379.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m140.2/140.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#@title Instalando o pacote LangChain\n",
        "!pip install langchain -q U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJvs5RTcbE64",
        "outputId": "46dac140-7c5e-4746-db0c-4d2a10dcde8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2.12\n"
          ]
        }
      ],
      "source": [
        "#@title Vers√£o do LangChain\n",
        "\n",
        "import langchain\n",
        "\n",
        "print(langchain.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Integra√ß√£o com o pacote da OpenAI\n",
        "\n",
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "8klfbjqKbUpp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20eabb00-0a6a-441b-bb66-5a1b7479cf50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/48.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m360.4/360.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM5RnKClbPtQ",
        "outputId": "0902a452-81c8-4a11-cc17-0b41b9caaca8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "#@title Definindo a chave da API da OpenAI\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extra√ß√£o de Informa√ß√£o**"
      ],
      "metadata": {
        "id": "48inXIgDTxlv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extra√ß√£o de Informa√ß√£o** (EI) √© o processo de identificar e extrair dados estruturados a partir de texto n√£o estruturado. O objetivo √© transformar o texto bruto em informa√ß√µes organizadas que podem ser facilmente analisadas e usadas para outras tarefas, como a constru√ß√£o de bases de dados ou a realiza√ß√£o de an√°lises."
      ],
      "metadata": {
        "id": "_OfEFH6c4hjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Construir uma cadeia de extra√ß√£o**"
      ],
      "metadata": {
        "id": "-lpmLOvr7s7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Schema***\n",
        "\n",
        "Primeiro, precisamos descrever quais informa√ß√µes queremos extrair do texto.\n",
        "\n",
        "Usaremos o `Pydantic` para definir um esquema de exemplo para extrair informa√ß√µes pessoais."
      ],
      "metadata": {
        "id": "ln2MlIfq77_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "class Pessoa(BaseModel):\n",
        "    \"\"\"Informa√ß√µes sobre uma pessoa.\"\"\"\n",
        "\n",
        "    # ^ Doc-string para a entidade Pessoa.\n",
        "    # Este doc-string √© enviado ao LLM como a descri√ß√£o do esquema Pessoa,\n",
        "    # e pode ajudar a melhorar os resultados da extra√ß√£o.\n",
        "\n",
        "    # Observe que:\n",
        "    # 1. Cada campo √© `opcional` -- isso permite que o modelo opte por n√£o extra√≠-lo!\n",
        "    # 2. Cada campo tem uma `descri√ß√£o` -- essa descri√ß√£o √© usada pelo LLM.\n",
        "    # Ter uma boa descri√ß√£o pode ajudar a melhorar os resultados da extra√ß√£o.\n",
        "    nome: Optional[str] = Field(default=None, description=\"O nome da pessoa\")\n",
        "    cor_cabelo: Optional[str] = Field(\n",
        "        default=None, description=\"A cor do cabelo da pessoa, se conhecida\"\n",
        "    )\n",
        "    altura_metros: Optional[str] = Field(\n",
        "        default=None, description=\"Altura medida em metros\"\n",
        "    )"
      ],
      "metadata": {
        "id": "eX2GKP2c8POV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui est√£o duas boas pr√°ticas ao definir um esquema:\n",
        "\n",
        "- Documente os atributos e o pr√≥prio esquema: essas informa√ß√µes s√£o enviadas ao LLM e usadas para melhorar a qualidade da extra√ß√£o de informa√ß√µes.\n",
        "- N√£o force o LLM a inventar informa√ß√µes! Acima, usamos `Optional` para os atributos, permitindo que o LLM retorne `None` se n√£o souber a resposta."
      ],
      "metadata": {
        "id": "ss2ElouD89N0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extrator**"
      ],
      "metadata": {
        "id": "OUdUpssO9Z_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos criar um extrator de informa√ß√µes usando o esquema definido acima."
      ],
      "metadata": {
        "id": "4OcmsitU9bwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "# Defina um prompt personalizado para fornecer instru√ß√µes e qualquer contexto adicional.\n",
        "# 1) Voc√™ pode adicionar exemplos ao modelo de prompt para melhorar a qualidade da extra√ß√£o.\n",
        "# 2) Introduza par√¢metros adicionais para levar o contexto em considera√ß√£o (por exemplo, incluir metadados\n",
        "#    sobre o documento do qual o texto foi extra√≠do.)\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Voc√™ √© um algoritmo de extra√ß√£o especializado. \"\n",
        "            \"Extraia apenas as informa√ß√µes relevantes do texto. \"\n",
        "            \"Se voc√™ n√£o souber o valor de um atributo solicitado para extra√ß√£o, \"\n",
        "            \"retorne nulo para o valor do atributo.\",\n",
        "        ),\n",
        "        (\"human\", \"{texto}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "unMCaSKD9m_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precisamos usar um modelo que suporte chamadas de fun√ß√µes/`ferramentas`."
      ],
      "metadata": {
        "id": "_fM9ynH8-BMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "chain = prompt | modelo.with_structured_output( schema = Pessoa )"
      ],
      "metadata": {
        "id": "Jun5BBQK-D02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos testar isso:"
      ],
      "metadata": {
        "id": "ENDtnrt6-k_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"Alan Smith tem 183 cent√≠metros de altura e cabelo loiro.\"\n",
        "\n",
        "chain.invoke({\"texto\": texto})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2JTybn6-ift",
        "outputId": "a80f296f-acc5-4bb2-cfd2-978099e58573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pessoa(nome='Alan Smith', cor_cabelo='loiro', altura_metros='1.83')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interessante**\n",
        "\n",
        ">\n",
        "\n",
        "Extra√ß√£o √© Generativa ü§Ø\n",
        ">\n",
        "Os LLMs s√£o modelos generativos, ent√£o eles podem fazer coisas bastante interessantes, como extrair corretamente a altura da pessoa em metros, mesmo que tenha sido fornecida em outra unidade!"
      ],
      "metadata": {
        "id": "zSP3k9db-_q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**M√∫ltiplas Entidades**"
      ],
      "metadata": {
        "id": "76kyr3u6_ui-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na maioria dos casos, voc√™ deve estar extraindo uma lista de entidades em vez de uma √∫nica entidade. Isso pode ser facilmente alcan√ßado usando o `Pydantic` ao aninhar modelos dentro de outros modelos."
      ],
      "metadata": {
        "id": "JUtLHhHJ_wsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "class Pessoa(BaseModel):\n",
        "    \"\"\"Informa√ß√µes sobre uma pessoa.\"\"\"\n",
        "\n",
        "    # ^ Doc-string para a entidade Pessoa.\n",
        "    # Este doc-string √© enviado ao LLM como a descri√ß√£o do esquema Pessoa,\n",
        "    # e pode ajudar a melhorar os resultados da extra√ß√£o.\n",
        "\n",
        "    # Observe que:\n",
        "    # 1. Cada campo √© `opcional` -- isso permite que o modelo opte por n√£o extra√≠-lo!\n",
        "    # 2. Cada campo tem uma `descri√ß√£o` -- essa descri√ß√£o √© usada pelo LLM.\n",
        "    # Ter uma boa descri√ß√£o pode ajudar a melhorar os resultados da extra√ß√£o.\n",
        "    nome: Optional[str] = Field(default=None, description=\"O nome da pessoa\")\n",
        "    cor_cabelo: Optional[str] = Field(\n",
        "        default=None, description=\"A cor do cabelo da pessoa, se conhecida\"\n",
        "    )\n",
        "    altura_metros: Optional[str] = Field(\n",
        "        default=None, description=\"Altura medida em metros\"\n",
        "    )\n",
        "\n",
        "class Dados(BaseModel):\n",
        "    \"\"\"Dados extra√≠dos sobre pessoas.\"\"\"\n",
        "\n",
        "    # Cria um modelo para que possamos extrair v√°rias entidades.\n",
        "    pessoas: List[Pessoa]"
      ],
      "metadata": {
        "id": "cnjE4iz4_93U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | modelo.with_structured_output( schema = Dados)\n",
        "\n",
        "texto = \"Meu nome √© Jo√£o, meu cabelo √© preto e eu tenho 1,83 metros de altura. Ana tem a mesma cor de cabelo que eu.\"\n",
        "\n",
        "chain.invoke({\"texto\": texto})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQMlwIemA2aI",
        "outputId": "c40f6482-0380-4a5b-de0d-d9e36103ff5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dados(pessoas=[Pessoa(nome='Jo√£o', cor_cabelo='preto', altura_metros='1,83'), Pessoa(nome='Ana', cor_cabelo='preto', altura_metros=None)])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Como lidar com textos longos ao realizar a extra√ß√£o**"
      ],
      "metadata": {
        "id": "i9aYk7O4R5-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quando trabalhar com arquivos, como PDFs, √© prov√°vel que voc√™ encontre texto que excede a janela de contexto do seu modelo de linguagem. Para processar esse texto, considere estas estrat√©gias:\n",
        "\n",
        "1. **Mudar o LLM**: escolha um LLM diferente que suporte uma janela de contexto maior.\n",
        "2. **For√ßa Bruta**: divida o documento em partes e extraia o conte√∫do de cada parte.\n",
        "3. **RAG (*Retrieval-Augmented Generation*)**: divida o documento em partes, indexe as partes e extraia conte√∫do apenas de um subconjunto de partes que pare√ßam \"relevantes\".\n",
        "\n",
        "Lembre-se de que essas estrat√©gias t√™m diferentes compromissos, e a melhor estrat√©gia provavelmente depende da aplica√ß√£o que voc√™ est√° projetando!\n",
        "\n",
        "Aqui iremos demonstrar como implementar as estrat√©gias 2 e 3."
      ],
      "metadata": {
        "id": "UcbCbEhnbD9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain_community"
      ],
      "metadata": {
        "id": "MtlqBDrZw0Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precisamos de alguns dados de exemplo! Vamos baixar um artigo sobre carros da **Wikipedia** e carreg√°-lo como um `Document` do LangChain."
      ],
      "metadata": {
        "id": "mBT8Br5sbqI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "import requests\n",
        "from langchain_community.document_loaders import BSHTMLLoader\n",
        "\n",
        "response = requests.get(\"https://en.wikipedia.org/wiki/Car\")\n",
        "\n",
        "with open(\"carros.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(response.text)\n",
        "\n",
        "loader = BSHTMLLoader(\"carros.html\")\n",
        "documento = loader.load()[0]\n",
        "documento.page_content = re.sub(\"\\n\\n+\", \"\\n\", documento.page_content)"
      ],
      "metadata": {
        "id": "e6wsKEamwwpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(documento.page_content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mM0Q7J6Yw-96",
        "outputId": "564b38b9-d06d-4104-dcf4-e20157f1858b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definir o esquema**"
      ],
      "metadata": {
        "id": "VQAqWNnlcYrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos o `Pydantic` para definir o esquema das informa√ß√µes que desejamos extrair. Neste caso, extrair uma lista de principais desenvolvimentos (por exemplo, eventos hist√≥ricos importantes) que incluam um ano e uma descri√ß√£o.\n",
        "\n",
        ">\n",
        "Observe que tamb√©m inclu√≠mos uma chave de evid√™ncia e instru√≠mos o modelo a fornecer, literalmente, as senten√ßas relevantes do texto do artigo. Isso nos permite comparar os resultados da extra√ß√£o com (a reconstru√ß√£o do modelo de) texto do documento original."
      ],
      "metadata": {
        "id": "YFGYT7dIcfA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Desenvolvimento(BaseModel):\n",
        "    \"\"\"Informa√ß√µes sobre um desenvolvimento na hist√≥ria dos carros.\"\"\"\n",
        "\n",
        "    ano: int = Field(\n",
        "        ..., description=\"O ano em que houve um desenvolvimento hist√≥rico importante.\"\n",
        "    )\n",
        "    descricao: str = Field(\n",
        "        ..., description=\"O que aconteceu neste ano? Qual foi o desenvolvimento?\"\n",
        "    )\n",
        "    evidencia: str = Field(\n",
        "        ...,\n",
        "        description=\"Repita literalmente a(s) frase(s) da qual as informa√ß√µes de ano e descri√ß√£o foram extra√≠das.\",\n",
        "    )\n",
        "\n",
        "\n",
        "class ExtracaoDados(BaseModel):\n",
        "    \"\"\"Informa√ß√µes extra√≠das sobre desenvolvimentos chave na hist√≥ria dos carros.\"\"\"\n",
        "\n",
        "    desenvolvimentos: List[Desenvolvimento]\n",
        "\n",
        "\n",
        "# Defina um prompt personalizado para fornecer instru√ß√µes e qualquer contexto adicional.\n",
        "# 1) Voc√™ pode adicionar exemplos no template do prompt para melhorar a qualidade da extra√ß√£o.\n",
        "# 2) Introduza par√¢metros adicionais para considerar o contexto (por exemplo, incluir metadados\n",
        "#    sobre o documento do qual o texto foi extra√≠do.)\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Voc√™ √© um especialista em identificar desenvolvimentos hist√≥ricos chave em textos. \"\n",
        "            \"Extraia apenas desenvolvimentos hist√≥ricos importantes. N√£o extraia nada se n√£o houver informa√ß√µes importantes no texto.\",\n",
        "        ),\n",
        "        (\"human\", \"{texto}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "YW0OV_V_c7r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Criar um extrator**"
      ],
      "metadata": {
        "id": "umnkysVseJfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos selecionar um LLM. Como estamos usando chamadas de `ferramentas`, precisaremos de um modelo que suporte esse recurso."
      ],
      "metadata": {
        "id": "ZDsLPdaJeMLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
      ],
      "metadata": {
        "id": "9ODWnJSteXrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extrator = prompt | modelo.with_structured_output(\n",
        "    schema = ExtracaoDados,\n",
        "    include_raw = False,\n",
        ")"
      ],
      "metadata": {
        "id": "5KABwirYeXto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Abordagem de for√ßa bruta**\n",
        "\n"
      ],
      "metadata": {
        "id": "nuxya9o3fG1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divida os documentos em partes (*chunks*) de modo que cada parte se encaixe na janela de contexto dos LLMs."
      ],
      "metadata": {
        "id": "pHZ5FhDTfKHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import TokenTextSplitter\n",
        "\n",
        "text_splitter = TokenTextSplitter(\n",
        "    chunk_size=2000,\n",
        "    chunk_overlap=20,\n",
        ")\n",
        "\n",
        "textos = text_splitter.split_text(documento.page_content)"
      ],
      "metadata": {
        "id": "Y32AiFb_esRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `chunk_size=2000`: define o tamanho m√°ximo de cada parte (*chunk*) em tokens. Neste caso, cada parte ter√° no m√°ximo 2000 tokens.\n",
        "\n",
        "* `chunk_overlap=20`: define o n√∫mero de tokens que se sobrep√µem entre partes adjacentes. Uma sobreposi√ß√£o de 20 tokens significa que os 20 tokens finais de uma parte ser√£o inclu√≠dos no in√≠cio da pr√≥xima parte. Isso ajuda a garantir que o contexto n√£o seja perdido entre partes consecutivas."
      ],
      "metadata": {
        "id": "U5c9TxefgAup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a funcionalidade de processamento em lote (`batch`) para executar a extra√ß√£o em paralelo em cada parte!"
      ],
      "metadata": {
        "id": "ddv3gEkigiqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Limite apenas √†s primeiras 3 partes\n",
        "# para que o c√≥digo possa ser executado rapidamente\n",
        "partes = textos[:3]\n",
        "\n",
        "resposta = extrator.batch(\n",
        "    [{\"texto\": texto} for texto in partes],\n",
        "    {\"max_concurrency\": 5},  # n√∫mero m√°ximo de chamadas simult√¢neas\n",
        ")"
      ],
      "metadata": {
        "id": "LsJfOmuPgnZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mesclar resultados**\n",
        "\n",
        "Ap√≥s extrair dados das partes, ser√° necess√°rio mesclar as extra√ß√µes."
      ],
      "metadata": {
        "id": "xYdCWtAyhNMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "desenvolvimentos = []\n",
        "\n",
        "for extracao in resposta:\n",
        "    desenvolvimentos.extend(extracao.desenvolvimentos)\n",
        "\n",
        "desenvolvimentos[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4dAz4DthLye",
        "outputId": "91066820-3a72-4c42-ca09-b66c2cef7cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Desenvolvimento(ano=1966, descricao='O Toyota Corolla come√ßou a ser produzido.', evidencia='The Toyota Corolla, which has been in production since 1966, is the best-selling series.'),\n",
              " Desenvolvimento(ano=1769, descricao='O inventor franc√™s Nicolas-Joseph Cugnot construiu o primeiro ve√≠culo rodovi√°rio movido a vapor.', evidencia='The French inventor Nicolas-Joseph Cugnot built the first steam-powered road vehicle in 1769.'),\n",
              " Desenvolvimento(ano=1808, descricao='O inventor su√≠√ßo Fran√ßois Isaac de Rivaz projetou e construiu o primeiro autom√≥vel movido a combust√£o interna.', evidencia='the Swiss inventor Fran√ßois Isaac de Rivaz designed and constructed the first internal combustion-powered automobile in 1808.'),\n",
              " Desenvolvimento(ano=1886, descricao='Carl Benz patenteou seu Benz Patent-Motorwagen, considerado o primeiro carro moderno.', evidencia='the modern car‚Äîa practical, marketable automobile for everyday use‚Äîwas invented in 1886, when the German inventor Carl Benz patented his Benz Patent-Motorwagen.'),\n",
              " Desenvolvimento(ano=1908, descricao='O Ford Model T foi iniciado, tornando-se um dos primeiros carros acess√≠veis em massa.', evidencia='One of the first cars affordable by the masses was the Ford Model T, begun in 1908.'),\n",
              " Desenvolvimento(ano=1881, descricao='O inventor franc√™s Gustave Trouv√© demonstrou um carro de tr√™s rodas movido a eletricidade.', evidencia='In November 1881, French inventor Gustave Trouv√© demonstrated a three-wheeled car powered by electricity at the International Exposition of Electricity.'),\n",
              " Desenvolvimento(ano=1879, descricao='Benz recebeu uma patente para seu primeiro motor, que foi projetado em 1878.', evidencia='In 1879, Benz was granted a patent for his first engine, which had been designed in 1878.'),\n",
              " Desenvolvimento(ano=1888, descricao=\"Bertha Benz undertook the first road trip by car to prove the road-worthiness of her husband's invention.\", evidencia=\"In August 1888, Bertha Benz, the wife and business partner of Carl Benz, undertook the first road trip by car, to prove the road-worthiness of her husband's invention.\"),\n",
              " Desenvolvimento(ano=1896, descricao='Benz designed and patented the first internal-combustion flat engine, called boxermotor.', evidencia='In 1896, Benz designed and patented the first internal-combustion flat engine, called boxermotor.'),\n",
              " Desenvolvimento(ano=1890, descricao='Daimler and Maybach founded Daimler Motoren Gesellschaft (DMG) in Cannstatt.', evidencia='Daimler and Maybach founded Daimler Motoren Gesellschaft (DMG) in Cannstatt in 1890.')]"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for desenvolvimento in sorted(desenvolvimentos, key=lambda x: x.ano):\n",
        "    print(f\"{desenvolvimento.ano}: {desenvolvimento.descricao}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sj8kR8U0hqzA",
        "outputId": "ae82168e-b583-45ea-fecf-658ab5aff296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1769: O inventor franc√™s Nicolas-Joseph Cugnot construiu o primeiro ve√≠culo rodovi√°rio movido a vapor.\n",
            "1808: O inventor su√≠√ßo Fran√ßois Isaac de Rivaz projetou e construiu o primeiro autom√≥vel movido a combust√£o interna.\n",
            "1879: Benz recebeu uma patente para seu primeiro motor, que foi projetado em 1878.\n",
            "1881: O inventor franc√™s Gustave Trouv√© demonstrou um carro de tr√™s rodas movido a eletricidade.\n",
            "1886: Carl Benz patenteou seu Benz Patent-Motorwagen, considerado o primeiro carro moderno.\n",
            "1888: Bertha Benz undertook the first road trip by car to prove the road-worthiness of her husband's invention.\n",
            "1890: Daimler and Maybach founded Daimler Motoren Gesellschaft (DMG) in Cannstatt.\n",
            "1892: Daimler sold their first car under the brand name Daimler.\n",
            "1892: Rudolf Diesel was granted a patent for a 'New Rational Combustion Engine'.\n",
            "1893: The first running, petrol-driven American car was built and road-tested by the Duryea brothers.\n",
            "1895: Selden was granted a US patent for a two-stroke car engine.\n",
            "1896: Benz designed and patented the first internal-combustion flat engine, called boxermotor.\n",
            "1896: The first production vehicles in Great Britain came from the Daimler Company.\n",
            "1901: Ransom E. Olds founded Olds Motor Vehicle Company (Oldsmobile).\n",
            "1908: O Ford Model T foi iniciado, tornando-se um dos primeiros carros acess√≠veis em massa.\n",
            "1913: Henry Ford began the world's first moving assembly line for cars.\n",
            "1914: An assembly line worker could buy a Model T with four months' pay.\n",
            "1921: Citro√´n was the first native European manufacturer to adopt the production method.\n",
            "1930: By 1930, 250 companies which did not have assembly lines had disappeared.\n",
            "1966: O Toyota Corolla come√ßou a ser produzido.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Abordagem baseada em *RAG***"
      ],
      "metadata": {
        "id": "3lpUooYIicQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***RAG*** (*Retrieval-Augmented Generation* - Gera√ß√£o aumentada de recupera√ß√£o)"
      ],
      "metadata": {
        "id": "VryuJbWgqgFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outra ideia simples √© dividir o texto em partes, mas em vez de extrair informa√ß√µes de cada parte, concentre-se apenas nas partes mais relevantes."
      ],
      "metadata": {
        "id": "lsrI6FvxmIFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Aten√ß√£o**\n",
        "\n",
        "Pode ser dif√≠cil identificar quais partes s√£o relevantes.\n",
        "\n",
        "Por exemplo, no artigo sobre carros que estamos usando aqui, a maior parte do artigo cont√©m informa√ß√µes sobre desenvolvimentos chave. Ent√£o, ao usar ***RAG***, provavelmente estaremos descartando muitas informa√ß√µes relevantes.\n",
        "\n",
        "Sugerimos experimentar com seu caso de uso e determinar se essa abordagem funciona ou n√£o.\n",
        "\n",
        "Para implementar a abordagem baseada em ***RAG***:\n",
        "\n",
        "1. Divida seu(s) documento(s) em partes e indexe-os (por exemplo, em um `vetorstore`);\n",
        "2. Adicione um passo de recupera√ß√£o antes do extrator, utilizando o `vetorstore`."
      ],
      "metadata": {
        "id": "W52plXzTiCH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***RAG*** (*Retrieval-Augmented Generation*) √© uma abordagem que combina recupera√ß√£o de informa√ß√µes com gera√ß√£o de texto. A ideia principal √© melhorar a capacidade de um modelo de linguagem ao integrar informa√ß√µes recuperadas de uma base de dados ou √≠ndice com a gera√ß√£o de texto."
      ],
      "metadata": {
        "id": "A6gHJpDfllgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como funciona o ***RAG***:\n",
        "\n",
        "   * **Recupera√ß√£o**: quando um modelo precisa gerar uma resposta ou extrair informa√ß√µes, primeiro ele realiza uma busca em uma base de dados ou √≠ndice para recuperar partes relevantes do texto ou documentos que podem ajudar a responder a pergunta ou completar a tarefa. Esta base de dados √© frequentemente um `vetorstore`, que armazena *embeddings* de texto para r√°pida recupera√ß√£o.\n",
        "\n",
        "   * **Gera√ß√£o**: ap√≥s recuperar as informa√ß√µes relevantes, o modelo de linguagem utiliza essas informa√ß√µes como contexto adicional para gerar uma resposta mais informada e precisa. Isso combina o conhecimento do modelo com dados espec√≠ficos recuperados, aumentando a relev√¢ncia e precis√£o da sa√≠da gerada."
      ],
      "metadata": {
        "id": "uNYY89Qflx-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***FAISS*** (*Facebook AI Similarity Search*) √© uma biblioteca desenvolvida pelo Facebook AI Research para realizar busca eficiente e similaridade de alta dimens√£o em grandes conjuntos de dados. √â projetada para lidar com grandes volumes de dados e facilitar a recupera√ß√£o de informa√ß√µes baseadas em similaridade."
      ],
      "metadata": {
        "id": "XyeWNHnameDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***FAISS*** √© especializado em encontrar vetores (representa√ß√µes num√©ricas de dados) similares a um vetor de consulta."
      ],
      "metadata": {
        "id": "nDV39Auimo6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Vectorstore*** √© um termo usado para descrever um tipo de armazenamento especializado em vetores, frequentemente utilizado em sistemas de recupera√ß√£o de informa√ß√µes e modelos de aprendizado de m√°quina. Ele √© projetado para gerenciar e buscar vetores de alta dimens√£o de maneira eficiente."
      ],
      "metadata": {
        "id": "XKuF39_gm0sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU FAISS"
      ],
      "metadata": {
        "id": "TlvPTjpBm918"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui est√° um exemplo simples que utiliza o `vetorstore` `FAISS`."
      ],
      "metadata": {
        "id": "kVWrXJiGnc8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "textos = text_splitter.split_text(documento.page_content)\n",
        "vectorstore = FAISS.from_texts(textos, embedding = OpenAIEmbeddings())\n",
        "\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_kwargs={\"k\": 1}\n",
        ")  # extrai apenas do primeiro documento"
      ],
      "metadata": {
        "id": "kbwZ3KU8h_uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste caso, o extrator ***RAG*** est√° analisando apenas o principal documento."
      ],
      "metadata": {
        "id": "lr3WVRPnnk4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_extrator = {\n",
        "    \"texto\": retriever | (lambda docs: docs[0].page_content)\n",
        "} | extrator"
      ],
      "metadata": {
        "id": "caWVSiHnnm5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = rag_extrator.invoke(\"Principais desenvolvimentos associados aos carros\")"
      ],
      "metadata": {
        "id": "96CqLaHrn1SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for desenvolvimento in resposta.desenvolvimentos:\n",
        "    print(desenvolvimento)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd3xtBMooLex",
        "outputId": "814ee8c6-320c-46a4-b248-88181f82ac2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ano=2006 descricao='Crescimento de dois d√≠gitos na receita e no n√∫mero de membros de servi√ßos de compartilhamento de carros nos EUA.' evidencia='...alguns servi√ßos de compartilhamento de carros tiveram crescimento de dois d√≠gitos na receita e no n√∫mero de membros entre 2006 e 2007.'\n",
            "ano=2020 descricao='Produ√ß√£o de 56 milh√µes de carros em todo o mundo, uma queda em rela√ß√£o a 67 milh√µes no ano anterior.' evidencia='Em 2020, havia 56 milh√µes de carros fabricados em todo o mundo, down from 67 million the previous year.'\n",
            "ano=2020 descricao='A China produziu 20 milh√µes de carros, liderando a produ√ß√£o mundial.' evidencia='A ind√∫stria automotiva na China produz de longe o maior n√∫mero (20 milh√µes em 2020)...'\n",
            "ano=2020 descricao='Cerca de um bilh√£o de carros est√£o nas estradas ao redor do mundo.' evidencia='Ao redor do mundo, h√° cerca de um bilh√£o de carros nas estradas.'\n",
            "ano=2019 descricao='Aumento da popularidade de sistemas de compartilhamento de bicicletas em cidades europeias e nos EUA.' evidencia='Sistemas de compartilhamento de bicicletas foram estabelecidos na China e em muitas cidades europeias, incluindo Copenhague e Amsterd√£.'\n",
            "ano=2019 descricao='Estudo sobre os benef√≠cios de introduzir bairros de baixo tr√°fego em Londres, mostrando que os benef√≠cios superam os custos em 100 vezes nos primeiros 20 anos.' evidencia='Um estudo que verificou os custos e os benef√≠cios de introduzir Low Traffic Neighbourhood em Londres encontrou que os benef√≠cios superam os custos aproximadamente por 100 vezes nos primeiros 20 anos.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for desenvolvimento in sorted(resposta.desenvolvimentos, key=lambda x: x.ano):\n",
        "    print(f\"{desenvolvimento.ano}: {desenvolvimento.descricao}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VfP7lQFoCFb",
        "outputId": "b66f2170-4057-4ec9-838f-84b45ec8fd2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2006: Crescimento de dois d√≠gitos na receita e no n√∫mero de membros de servi√ßos de compartilhamento de carros nos EUA.\n",
            "2019: Aumento da popularidade de sistemas de compartilhamento de bicicletas em cidades europeias e nos EUA.\n",
            "2019: Estudo sobre os benef√≠cios de introduzir bairros de baixo tr√°fego em Londres, mostrando que os benef√≠cios superam os custos em 100 vezes nos primeiros 20 anos.\n",
            "2020: Produ√ß√£o de 56 milh√µes de carros em todo o mundo, uma queda em rela√ß√£o a 67 milh√µes no ano anterior.\n",
            "2020: A China produziu 20 milh√µes de carros, liderando a produ√ß√£o mundial.\n",
            "2020: Cerca de um bilh√£o de carros est√£o nas estradas ao redor do mundo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE:**\n",
        "\n",
        "M√©todos diferentes t√™m suas pr√≥prias vantagens e desvantagens relacionadas a custo, velocidade e precis√£o.\n",
        "\n",
        "Fique atento a esses problemas:\n",
        "\n",
        "- Dividir o conte√∫do em partes significa que o LLM pode falhar em extrair informa√ß√µes se essas informa√ß√µes estiverem espalhadas por v√°rios fragmentos.\n",
        "- Um grande sobreposi√ß√£o entre os fragmentos pode fazer com que a mesma informa√ß√£o seja extra√≠da duas vezes, ent√£o esteja preparado para desduplicar!\n",
        "- LLMs podem inventar dados. Se voc√™ estiver procurando por um √∫nico fato em um texto longo e usar uma abordagem de for√ßa bruta, pode acabar obtendo mais dados inventados."
      ],
      "metadata": {
        "id": "Q9pm-gJzoyeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Como usar apenas o *prompting* para realizar a extra√ß√£o**"
      ],
      "metadata": {
        "id": "H6T0Amt3TRRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recursos de chamada de `ferramentas` n√£o s√£o necess√°rios para gerar sa√≠das estruturadas a partir de LLMs. LLMs que conseguem seguir bem as instru√ß√µes do *prompt* podem ser encarregados de fornecer informa√ß√µes em um formato espec√≠fico.\n",
        ">\n",
        "Essa abordagem depende de projetar bons *prompts* e, em seguida, fazer o *parser* da sa√≠da dos LLMs para garantir que eles extra√≠am informa√ß√µes de forma eficaz.\n",
        ">\n",
        "Para extrair dados sem recursos de chamada de `ferramentas`:\n",
        ">\n",
        "1. Instrua o LLM a gerar texto seguindo um formato esperado (por exemplo, `JSON` com um determinado esquema);\n",
        "2. Use *parsers* de sa√≠da para estruturar a resposta do modelo em um objeto Python desejado."
      ],
      "metadata": {
        "id": "U3ra5F8PU4cE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nem todos os modelos suportam `.with_structured_output()`, j√° que nem todos os modelos t√™m suporte para chamadas de `ferramentas` ou modo `JSON`. Para esses modelos, voc√™ precisar√° solicitar diretamente ao modelo que use um formato espec√≠fico e usar um *parser* de sa√≠da para extrair a resposta estruturada da sa√≠da bruta do modelo.\n"
      ],
      "metadata": {
        "id": "JwiTIHxC6go0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Usando `PydanticOutputParser`**"
      ],
      "metadata": {
        "id": "EuhAtiWJ66MI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O exemplo a seguir usa o `PydanticOutputParser` embutido para fazer o *parser* da sa√≠da de um modelo de *chat* solicitado a corresponder ao esquema `Pydantic` fornecido. Note que estamos adicionando `format_instructions` diretamente ao *prompt* a partir de um m√©todo no *parser*:"
      ],
      "metadata": {
        "id": "shiUhUba6_xD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "class Pessoa(BaseModel):\n",
        "    \"\"\"Informa√ß√£o sobre uma pessoa.\"\"\"\n",
        "\n",
        "    nome: str = Field(..., description=\"O nome da pessoa.\")\n",
        "    altura_metros: float = Field(\n",
        "        ..., description=\"A altura da pessoa expressa em metros.\"\n",
        "    )\n",
        "\n",
        "class Pessoas(BaseModel):\n",
        "    \"\"\"Informa√ß√µes de identifica√ß√£o sobre todas as pessoas em um texto.\"\"\"\n",
        "\n",
        "    pessoas: List[Pessoa]\n",
        "\n",
        "# parser\n",
        "parser = PydanticOutputParser(pydantic_object=Pessoas)\n",
        "\n",
        "# prompt\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Responda √† consulta do usu√°rio. Envolva a sa√≠da em tags `json`\\n{format_instructions}\",\n",
        "        ),\n",
        "        (\"human\", \"{consulta}\"),\n",
        "    ]\n",
        ").partial(format_instructions = parser.get_format_instructions())"
      ],
      "metadata": {
        "id": "8iNncLbbTJB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos dar uma olhada em quais informa√ß√µes ser√£o enviadas ao modelo:"
      ],
      "metadata": {
        "id": "OMLEIvFQ-LkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "consulta = \"Ana tem 23 anos e mede 1,83 metros de altura.\"\n",
        "\n",
        "print(prompt.invoke(consulta).to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw46s5HbTM23",
        "outputId": "d80640e9-aabc-42b8-8342-8e5f33ac1af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: Responda √† consulta do usu√°rio. Envolva a sa√≠da em tags `json`\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"description\": \"Informa\\u00e7\\u00f5es de identifica\\u00e7\\u00e3o sobre todas as pessoas em um texto.\", \"properties\": {\"pessoas\": {\"title\": \"Pessoas\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Pessoa\"}}}, \"required\": [\"pessoas\"], \"definitions\": {\"Pessoa\": {\"title\": \"Pessoa\", \"description\": \"Informa\\u00e7\\u00e3o sobre uma pessoa.\", \"type\": \"object\", \"properties\": {\"nome\": {\"title\": \"Nome\", \"description\": \"O nome da pessoa.\", \"type\": \"string\"}, \"altura_metros\": {\"title\": \"Altura Metros\", \"description\": \"A altura da pessoa expressa em metros.\", \"type\": \"number\"}}, \"required\": [\"nome\", \"altura_metros\"]}}}\n",
            "```\n",
            "Human: Ana tem 23 anos e mede 1,83 metros de altura.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "E agora, vamos invoc√°-lo:"
      ],
      "metadata": {
        "id": "hB8IhwrJ-ZlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | modelo | parser\n",
        "\n",
        "resposta = chain.invoke({\"consulta\": consulta})"
      ],
      "metadata": {
        "id": "CEsllJoPTV1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAUEHVGS9o49",
        "outputId": "b40997e7-8a77-410f-f557-e39d88e8750a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pessoas(pessoas=[Pessoa(nome='Ana', altura_metros=1.83)])"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "_JlQcUbfWQ0T",
        "outputId": "7f634b11-d6b1-4163-8f7e-25b6939e8bc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.Pessoas"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>Pessoas</b><br/>def __init__(__pydantic_self__, **data: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\"></a>Informa√ß√µes de identifica√ß√£o sobre todas as pessoas em um texto.</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta.pessoas[0].nome"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "prAhp-AY9h6t",
        "outputId": "3e2b9463-a6f9-4662-d3b7-5d8bacb850ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ana'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for pessoa in resposta.pessoas:\n",
        "    print(pessoa.nome)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60oG2oihWeu_",
        "outputId": "afabb29c-0650-49a5-ac26-860548ec3a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ana\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Parser* Personalizado**"
      ],
      "metadata": {
        "id": "yKzFyD-j-mg5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voc√™ tamb√©m pode criar um *prompt* e um *parser* personalizados com a **Linguagem de Express√£o LangChain** (`LCEL`), usando uma fun√ß√£o simples para fazer o *parser* da sa√≠da do modelo:"
      ],
      "metadata": {
        "id": "1UAHjMqp-mnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "from langchain_core.messages import AIMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "class Pessoa(BaseModel):\n",
        "    \"\"\"Informa√ß√£o sobre uma pessoa.\"\"\"\n",
        "\n",
        "    nome: str = Field(..., description=\"O nome da pessoa.\")\n",
        "    altura_metros: float = Field(\n",
        "        ..., description=\"A altura da pessoa expressa em metros.\"\n",
        "    )\n",
        "\n",
        "class Pessoas(BaseModel):\n",
        "    \"\"\"Informa√ß√µes de identifica√ß√£o sobre todas as pessoas em um texto.\"\"\"\n",
        "\n",
        "    pessoas: List[Pessoa]\n",
        "\n",
        "# prompt\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Responda √† consulta do usu√°rio. Saia com sua resposta como JSON que  \"\n",
        "            \"corresponda ao esquema fornecido: json\\n{schema}\\n. \"\n",
        "            \"Certifique-se de envolver a resposta em tags json e .\",\n",
        "        ),\n",
        "        (\"human\", \"{consulta}\"),\n",
        "    ]\n",
        ").partial(schema=Pessoas.schema())\n",
        "\n",
        "\n",
        "# parser personalizado\n",
        "def extrair_json(mensagem: AIMessage) -> List[dict]:\n",
        "    \"\"\"Extraia o conte√∫do JSON de uma string em que o JSON est√° embutido entre as tags ```json e ```.\n",
        "\n",
        "    Par√¢metros:\n",
        "        texto (str): O texto contendo o conte√∫do JSON.\n",
        "\n",
        "    Retorna:\n",
        "        lista: Uma lista de strings JSON extra√≠das.\n",
        "    \"\"\"\n",
        "    texto = mensagem.content\n",
        "\n",
        "    # padr√£o da express√£o regular para corresponder a blocos JSON\n",
        "    padrao = r\"```json(.*?)```\"\n",
        "\n",
        "    # correspond√™ncias n√£o sobrepostas do padr√£o na string\n",
        "    matches = re.findall(padrao, texto, re.DOTALL)\n",
        "\n",
        "    # retorne a lista de strings JSON correspondentes, removendo quaisquer espa√ßos em branco no in√≠cio ou no final\n",
        "    try:\n",
        "        return [json.loads(match.strip()) for match in matches]\n",
        "    except Exception:\n",
        "        raise ValueError(f\"Falha no parser: {mensagem}\")"
      ],
      "metadata": {
        "id": "0XoG6qcxTf-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"Ana tem 23 anos e mede 1,83 metros de altura.\"\n",
        "\n",
        "print(prompt.format_prompt(consulta = texto).to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uXHH15DTjsM",
        "outputId": "26195622-37f7-4026-8d64-dd0111c7c320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: Responda √† consulta do usu√°rio. Saia com sua resposta como JSON que  corresponda ao esquema fornecido: json\n",
            "{'title': 'Pessoas', 'description': 'Informa√ß√µes de identifica√ß√£o sobre todas as pessoas em um texto.', 'type': 'object', 'properties': {'pessoas': {'title': 'Pessoas', 'type': 'array', 'items': {'$ref': '#/definitions/Pessoa'}}}, 'required': ['pessoas'], 'definitions': {'Pessoa': {'title': 'Pessoa', 'description': 'Informa√ß√£o sobre uma pessoa.', 'type': 'object', 'properties': {'nome': {'title': 'Nome', 'description': 'O nome da pessoa.', 'type': 'string'}, 'altura_metros': {'title': 'Altura Metros', 'description': 'A altura da pessoa expressa em metros.', 'type': 'number'}}, 'required': ['nome', 'altura_metros']}}}\n",
            ". Certifique-se de envolver a resposta em tags json e .\n",
            "Human: Ana tem 23 anos e mede 1,83 metros de altura.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | modelo | extrair_json\n",
        "\n",
        "resposta = chain.invoke({\"consulta\": texto})"
      ],
      "metadata": {
        "id": "pl-BoF91TmyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvhJGwmeB4ar",
        "outputId": "224c4006-38d3-4f9a-b732-8a7ad8608988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'pessoas': [{'nome': 'Ana', 'altura_metros': 1.83}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8DchUNmCpTh",
        "outputId": "8e6a757d-3f5a-4739-c1e8-89d19ea130cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta[0]['pessoas'][0]['nome']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "G9YqE9piB6NN",
        "outputId": "47003715-0ec3-4397-ea3a-ae7ca8ec5158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ana'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Refer√™ncias:**"
      ],
      "metadata": {
        "id": "oe14pd2FVNcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> https://python.langchain.com/v0.2/docs/tutorials/extraction/\n",
        "\n",
        "> https://python.langchain.com/v0.2/docs/how_to/extraction_long_text/\n",
        "\n",
        "> https://python.langchain.com/v0.2/docs/how_to/extraction_parse/"
      ],
      "metadata": {
        "id": "FgdLNhmITBCi"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-lpmLOvr7s7G",
        "H6T0Amt3TRRK",
        "oe14pd2FVNcC"
      ],
      "authorship_tag": "ABX9TyN4R5xxo7YsANlKBdYUmuyd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}