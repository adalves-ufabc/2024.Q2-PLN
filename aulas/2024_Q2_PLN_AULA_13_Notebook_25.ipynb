{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalves-ufabc/2024.Q2-PLN/blob/main/2024_Q2_PLN_AULA_13_Notebook_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2024-Q2]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmK05FgcOzL2"
      },
      "source": [
        "## **LangChain**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYKEbnlNTVlR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5ab4959-6a9b-4298-d162-3799a06d4fad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m950.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.6/990.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.9/379.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.2/140.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#@title Instalando o pacote LangChain\n",
        "!pip install langchain -q U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJvs5RTcbE64",
        "outputId": "46dac140-7c5e-4746-db0c-4d2a10dcde8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2.12\n"
          ]
        }
      ],
      "source": [
        "#@title Versão do LangChain\n",
        "\n",
        "import langchain\n",
        "\n",
        "print(langchain.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Integração com o pacote da OpenAI\n",
        "\n",
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "8klfbjqKbUpp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20eabb00-0a6a-441b-bb66-5a1b7479cf50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/48.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.4/360.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM5RnKClbPtQ",
        "outputId": "0902a452-81c8-4a11-cc17-0b41b9caaca8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "#@title Definindo a chave da API da OpenAI\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extração de Informação**"
      ],
      "metadata": {
        "id": "48inXIgDTxlv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extração de Informação** (EI) é o processo de identificar e extrair dados estruturados a partir de texto não estruturado. O objetivo é transformar o texto bruto em informações organizadas que podem ser facilmente analisadas e usadas para outras tarefas, como a construção de bases de dados ou a realização de análises."
      ],
      "metadata": {
        "id": "_OfEFH6c4hjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Construir uma cadeia de extração**"
      ],
      "metadata": {
        "id": "-lpmLOvr7s7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Schema***\n",
        "\n",
        "Primeiro, precisamos descrever quais informações queremos extrair do texto.\n",
        "\n",
        "Usaremos o `Pydantic` para definir um esquema de exemplo para extrair informações pessoais."
      ],
      "metadata": {
        "id": "ln2MlIfq77_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "class Pessoa(BaseModel):\n",
        "    \"\"\"Informações sobre uma pessoa.\"\"\"\n",
        "\n",
        "    # ^ Doc-string para a entidade Pessoa.\n",
        "    # Este doc-string é enviado ao LLM como a descrição do esquema Pessoa,\n",
        "    # e pode ajudar a melhorar os resultados da extração.\n",
        "\n",
        "    # Observe que:\n",
        "    # 1. Cada campo é `opcional` -- isso permite que o modelo opte por não extraí-lo!\n",
        "    # 2. Cada campo tem uma `descrição` -- essa descrição é usada pelo LLM.\n",
        "    # Ter uma boa descrição pode ajudar a melhorar os resultados da extração.\n",
        "    nome: Optional[str] = Field(default=None, description=\"O nome da pessoa\")\n",
        "    cor_cabelo: Optional[str] = Field(\n",
        "        default=None, description=\"A cor do cabelo da pessoa, se conhecida\"\n",
        "    )\n",
        "    altura_metros: Optional[str] = Field(\n",
        "        default=None, description=\"Altura medida em metros\"\n",
        "    )"
      ],
      "metadata": {
        "id": "eX2GKP2c8POV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui estão duas boas práticas ao definir um esquema:\n",
        "\n",
        "- Documente os atributos e o próprio esquema: essas informações são enviadas ao LLM e usadas para melhorar a qualidade da extração de informações.\n",
        "- Não force o LLM a inventar informações! Acima, usamos `Optional` para os atributos, permitindo que o LLM retorne `None` se não souber a resposta."
      ],
      "metadata": {
        "id": "ss2ElouD89N0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extrator**"
      ],
      "metadata": {
        "id": "OUdUpssO9Z_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos criar um extrator de informações usando o esquema definido acima."
      ],
      "metadata": {
        "id": "4OcmsitU9bwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "# Defina um prompt personalizado para fornecer instruções e qualquer contexto adicional.\n",
        "# 1) Você pode adicionar exemplos ao modelo de prompt para melhorar a qualidade da extração.\n",
        "# 2) Introduza parâmetros adicionais para levar o contexto em consideração (por exemplo, incluir metadados\n",
        "#    sobre o documento do qual o texto foi extraído.)\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Você é um algoritmo de extração especializado. \"\n",
        "            \"Extraia apenas as informações relevantes do texto. \"\n",
        "            \"Se você não souber o valor de um atributo solicitado para extração, \"\n",
        "            \"retorne nulo para o valor do atributo.\",\n",
        "        ),\n",
        "        (\"human\", \"{texto}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "unMCaSKD9m_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precisamos usar um modelo que suporte chamadas de funções/`ferramentas`."
      ],
      "metadata": {
        "id": "_fM9ynH8-BMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "chain = prompt | modelo.with_structured_output( schema = Pessoa )"
      ],
      "metadata": {
        "id": "Jun5BBQK-D02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos testar isso:"
      ],
      "metadata": {
        "id": "ENDtnrt6-k_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"Alan Smith tem 183 centímetros de altura e cabelo loiro.\"\n",
        "\n",
        "chain.invoke({\"texto\": texto})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2JTybn6-ift",
        "outputId": "a80f296f-acc5-4bb2-cfd2-978099e58573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pessoa(nome='Alan Smith', cor_cabelo='loiro', altura_metros='1.83')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interessante**\n",
        "\n",
        ">\n",
        "\n",
        "Extração é Generativa 🤯\n",
        ">\n",
        "Os LLMs são modelos generativos, então eles podem fazer coisas bastante interessantes, como extrair corretamente a altura da pessoa em metros, mesmo que tenha sido fornecida em outra unidade!"
      ],
      "metadata": {
        "id": "zSP3k9db-_q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Múltiplas Entidades**"
      ],
      "metadata": {
        "id": "76kyr3u6_ui-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na maioria dos casos, você deve estar extraindo uma lista de entidades em vez de uma única entidade. Isso pode ser facilmente alcançado usando o `Pydantic` ao aninhar modelos dentro de outros modelos."
      ],
      "metadata": {
        "id": "JUtLHhHJ_wsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "class Pessoa(BaseModel):\n",
        "    \"\"\"Informações sobre uma pessoa.\"\"\"\n",
        "\n",
        "    # ^ Doc-string para a entidade Pessoa.\n",
        "    # Este doc-string é enviado ao LLM como a descrição do esquema Pessoa,\n",
        "    # e pode ajudar a melhorar os resultados da extração.\n",
        "\n",
        "    # Observe que:\n",
        "    # 1. Cada campo é `opcional` -- isso permite que o modelo opte por não extraí-lo!\n",
        "    # 2. Cada campo tem uma `descrição` -- essa descrição é usada pelo LLM.\n",
        "    # Ter uma boa descrição pode ajudar a melhorar os resultados da extração.\n",
        "    nome: Optional[str] = Field(default=None, description=\"O nome da pessoa\")\n",
        "    cor_cabelo: Optional[str] = Field(\n",
        "        default=None, description=\"A cor do cabelo da pessoa, se conhecida\"\n",
        "    )\n",
        "    altura_metros: Optional[str] = Field(\n",
        "        default=None, description=\"Altura medida em metros\"\n",
        "    )\n",
        "\n",
        "class Dados(BaseModel):\n",
        "    \"\"\"Dados extraídos sobre pessoas.\"\"\"\n",
        "\n",
        "    # Cria um modelo para que possamos extrair várias entidades.\n",
        "    pessoas: List[Pessoa]"
      ],
      "metadata": {
        "id": "cnjE4iz4_93U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | modelo.with_structured_output( schema = Dados)\n",
        "\n",
        "texto = \"Meu nome é João, meu cabelo é preto e eu tenho 1,83 metros de altura. Ana tem a mesma cor de cabelo que eu.\"\n",
        "\n",
        "chain.invoke({\"texto\": texto})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQMlwIemA2aI",
        "outputId": "c40f6482-0380-4a5b-de0d-d9e36103ff5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dados(pessoas=[Pessoa(nome='João', cor_cabelo='preto', altura_metros='1,83'), Pessoa(nome='Ana', cor_cabelo='preto', altura_metros=None)])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Como lidar com textos longos ao realizar a extração**"
      ],
      "metadata": {
        "id": "i9aYk7O4R5-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quando trabalhar com arquivos, como PDFs, é provável que você encontre texto que excede a janela de contexto do seu modelo de linguagem. Para processar esse texto, considere estas estratégias:\n",
        "\n",
        "1. **Mudar o LLM**: escolha um LLM diferente que suporte uma janela de contexto maior.\n",
        "2. **Força Bruta**: divida o documento em partes e extraia o conteúdo de cada parte.\n",
        "3. **RAG (*Retrieval-Augmented Generation*)**: divida o documento em partes, indexe as partes e extraia conteúdo apenas de um subconjunto de partes que pareçam \"relevantes\".\n",
        "\n",
        "Lembre-se de que essas estratégias têm diferentes compromissos, e a melhor estratégia provavelmente depende da aplicação que você está projetando!\n",
        "\n",
        "Aqui iremos demonstrar como implementar as estratégias 2 e 3."
      ],
      "metadata": {
        "id": "UcbCbEhnbD9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain_community"
      ],
      "metadata": {
        "id": "MtlqBDrZw0Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precisamos de alguns dados de exemplo! Vamos baixar um artigo sobre carros da **Wikipedia** e carregá-lo como um `Document` do LangChain."
      ],
      "metadata": {
        "id": "mBT8Br5sbqI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "import requests\n",
        "from langchain_community.document_loaders import BSHTMLLoader\n",
        "\n",
        "response = requests.get(\"https://en.wikipedia.org/wiki/Car\")\n",
        "\n",
        "with open(\"carros.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(response.text)\n",
        "\n",
        "loader = BSHTMLLoader(\"carros.html\")\n",
        "documento = loader.load()[0]\n",
        "documento.page_content = re.sub(\"\\n\\n+\", \"\\n\", documento.page_content)"
      ],
      "metadata": {
        "id": "e6wsKEamwwpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(documento.page_content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mM0Q7J6Yw-96",
        "outputId": "564b38b9-d06d-4104-dcf4-e20157f1858b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definir o esquema**"
      ],
      "metadata": {
        "id": "VQAqWNnlcYrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos o `Pydantic` para definir o esquema das informações que desejamos extrair. Neste caso, extrair uma lista de principais desenvolvimentos (por exemplo, eventos históricos importantes) que incluam um ano e uma descrição.\n",
        "\n",
        ">\n",
        "Observe que também incluímos uma chave de evidência e instruímos o modelo a fornecer, literalmente, as sentenças relevantes do texto do artigo. Isso nos permite comparar os resultados da extração com (a reconstrução do modelo de) texto do documento original."
      ],
      "metadata": {
        "id": "YFGYT7dIcfA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Desenvolvimento(BaseModel):\n",
        "    \"\"\"Informações sobre um desenvolvimento na história dos carros.\"\"\"\n",
        "\n",
        "    ano: int = Field(\n",
        "        ..., description=\"O ano em que houve um desenvolvimento histórico importante.\"\n",
        "    )\n",
        "    descricao: str = Field(\n",
        "        ..., description=\"O que aconteceu neste ano? Qual foi o desenvolvimento?\"\n",
        "    )\n",
        "    evidencia: str = Field(\n",
        "        ...,\n",
        "        description=\"Repita literalmente a(s) frase(s) da qual as informações de ano e descrição foram extraídas.\",\n",
        "    )\n",
        "\n",
        "\n",
        "class ExtracaoDados(BaseModel):\n",
        "    \"\"\"Informações extraídas sobre desenvolvimentos chave na história dos carros.\"\"\"\n",
        "\n",
        "    desenvolvimentos: List[Desenvolvimento]\n",
        "\n",
        "\n",
        "# Defina um prompt personalizado para fornecer instruções e qualquer contexto adicional.\n",
        "# 1) Você pode adicionar exemplos no template do prompt para melhorar a qualidade da extração.\n",
        "# 2) Introduza parâmetros adicionais para considerar o contexto (por exemplo, incluir metadados\n",
        "#    sobre o documento do qual o texto foi extraído.)\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Você é um especialista em identificar desenvolvimentos históricos chave em textos. \"\n",
        "            \"Extraia apenas desenvolvimentos históricos importantes. Não extraia nada se não houver informações importantes no texto.\",\n",
        "        ),\n",
        "        (\"human\", \"{texto}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "YW0OV_V_c7r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Criar um extrator**"
      ],
      "metadata": {
        "id": "umnkysVseJfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos selecionar um LLM. Como estamos usando chamadas de `ferramentas`, precisaremos de um modelo que suporte esse recurso."
      ],
      "metadata": {
        "id": "ZDsLPdaJeMLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
      ],
      "metadata": {
        "id": "9ODWnJSteXrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extrator = prompt | modelo.with_structured_output(\n",
        "    schema = ExtracaoDados,\n",
        "    include_raw = False,\n",
        ")"
      ],
      "metadata": {
        "id": "5KABwirYeXto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Abordagem de força bruta**\n",
        "\n"
      ],
      "metadata": {
        "id": "nuxya9o3fG1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divida os documentos em partes (*chunks*) de modo que cada parte se encaixe na janela de contexto dos LLMs."
      ],
      "metadata": {
        "id": "pHZ5FhDTfKHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import TokenTextSplitter\n",
        "\n",
        "text_splitter = TokenTextSplitter(\n",
        "    chunk_size=2000,\n",
        "    chunk_overlap=20,\n",
        ")\n",
        "\n",
        "textos = text_splitter.split_text(documento.page_content)"
      ],
      "metadata": {
        "id": "Y32AiFb_esRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `chunk_size=2000`: define o tamanho máximo de cada parte (*chunk*) em tokens. Neste caso, cada parte terá no máximo 2000 tokens.\n",
        "\n",
        "* `chunk_overlap=20`: define o número de tokens que se sobrepõem entre partes adjacentes. Uma sobreposição de 20 tokens significa que os 20 tokens finais de uma parte serão incluídos no início da próxima parte. Isso ajuda a garantir que o contexto não seja perdido entre partes consecutivas."
      ],
      "metadata": {
        "id": "U5c9TxefgAup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a funcionalidade de processamento em lote (`batch`) para executar a extração em paralelo em cada parte!"
      ],
      "metadata": {
        "id": "ddv3gEkigiqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Limite apenas às primeiras 3 partes\n",
        "# para que o código possa ser executado rapidamente\n",
        "partes = textos[:3]\n",
        "\n",
        "resposta = extrator.batch(\n",
        "    [{\"texto\": texto} for texto in partes],\n",
        "    {\"max_concurrency\": 5},  # número máximo de chamadas simultâneas\n",
        ")"
      ],
      "metadata": {
        "id": "LsJfOmuPgnZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mesclar resultados**\n",
        "\n",
        "Após extrair dados das partes, será necessário mesclar as extrações."
      ],
      "metadata": {
        "id": "xYdCWtAyhNMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "desenvolvimentos = []\n",
        "\n",
        "for extracao in resposta:\n",
        "    desenvolvimentos.extend(extracao.desenvolvimentos)\n",
        "\n",
        "desenvolvimentos[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4dAz4DthLye",
        "outputId": "91066820-3a72-4c42-ca09-b66c2cef7cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Desenvolvimento(ano=1966, descricao='O Toyota Corolla começou a ser produzido.', evidencia='The Toyota Corolla, which has been in production since 1966, is the best-selling series.'),\n",
              " Desenvolvimento(ano=1769, descricao='O inventor francês Nicolas-Joseph Cugnot construiu o primeiro veículo rodoviário movido a vapor.', evidencia='The French inventor Nicolas-Joseph Cugnot built the first steam-powered road vehicle in 1769.'),\n",
              " Desenvolvimento(ano=1808, descricao='O inventor suíço François Isaac de Rivaz projetou e construiu o primeiro automóvel movido a combustão interna.', evidencia='the Swiss inventor François Isaac de Rivaz designed and constructed the first internal combustion-powered automobile in 1808.'),\n",
              " Desenvolvimento(ano=1886, descricao='Carl Benz patenteou seu Benz Patent-Motorwagen, considerado o primeiro carro moderno.', evidencia='the modern car—a practical, marketable automobile for everyday use—was invented in 1886, when the German inventor Carl Benz patented his Benz Patent-Motorwagen.'),\n",
              " Desenvolvimento(ano=1908, descricao='O Ford Model T foi iniciado, tornando-se um dos primeiros carros acessíveis em massa.', evidencia='One of the first cars affordable by the masses was the Ford Model T, begun in 1908.'),\n",
              " Desenvolvimento(ano=1881, descricao='O inventor francês Gustave Trouvé demonstrou um carro de três rodas movido a eletricidade.', evidencia='In November 1881, French inventor Gustave Trouvé demonstrated a three-wheeled car powered by electricity at the International Exposition of Electricity.'),\n",
              " Desenvolvimento(ano=1879, descricao='Benz recebeu uma patente para seu primeiro motor, que foi projetado em 1878.', evidencia='In 1879, Benz was granted a patent for his first engine, which had been designed in 1878.'),\n",
              " Desenvolvimento(ano=1888, descricao=\"Bertha Benz undertook the first road trip by car to prove the road-worthiness of her husband's invention.\", evidencia=\"In August 1888, Bertha Benz, the wife and business partner of Carl Benz, undertook the first road trip by car, to prove the road-worthiness of her husband's invention.\"),\n",
              " Desenvolvimento(ano=1896, descricao='Benz designed and patented the first internal-combustion flat engine, called boxermotor.', evidencia='In 1896, Benz designed and patented the first internal-combustion flat engine, called boxermotor.'),\n",
              " Desenvolvimento(ano=1890, descricao='Daimler and Maybach founded Daimler Motoren Gesellschaft (DMG) in Cannstatt.', evidencia='Daimler and Maybach founded Daimler Motoren Gesellschaft (DMG) in Cannstatt in 1890.')]"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for desenvolvimento in sorted(desenvolvimentos, key=lambda x: x.ano):\n",
        "    print(f\"{desenvolvimento.ano}: {desenvolvimento.descricao}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sj8kR8U0hqzA",
        "outputId": "ae82168e-b583-45ea-fecf-658ab5aff296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1769: O inventor francês Nicolas-Joseph Cugnot construiu o primeiro veículo rodoviário movido a vapor.\n",
            "1808: O inventor suíço François Isaac de Rivaz projetou e construiu o primeiro automóvel movido a combustão interna.\n",
            "1879: Benz recebeu uma patente para seu primeiro motor, que foi projetado em 1878.\n",
            "1881: O inventor francês Gustave Trouvé demonstrou um carro de três rodas movido a eletricidade.\n",
            "1886: Carl Benz patenteou seu Benz Patent-Motorwagen, considerado o primeiro carro moderno.\n",
            "1888: Bertha Benz undertook the first road trip by car to prove the road-worthiness of her husband's invention.\n",
            "1890: Daimler and Maybach founded Daimler Motoren Gesellschaft (DMG) in Cannstatt.\n",
            "1892: Daimler sold their first car under the brand name Daimler.\n",
            "1892: Rudolf Diesel was granted a patent for a 'New Rational Combustion Engine'.\n",
            "1893: The first running, petrol-driven American car was built and road-tested by the Duryea brothers.\n",
            "1895: Selden was granted a US patent for a two-stroke car engine.\n",
            "1896: Benz designed and patented the first internal-combustion flat engine, called boxermotor.\n",
            "1896: The first production vehicles in Great Britain came from the Daimler Company.\n",
            "1901: Ransom E. Olds founded Olds Motor Vehicle Company (Oldsmobile).\n",
            "1908: O Ford Model T foi iniciado, tornando-se um dos primeiros carros acessíveis em massa.\n",
            "1913: Henry Ford began the world's first moving assembly line for cars.\n",
            "1914: An assembly line worker could buy a Model T with four months' pay.\n",
            "1921: Citroën was the first native European manufacturer to adopt the production method.\n",
            "1930: By 1930, 250 companies which did not have assembly lines had disappeared.\n",
            "1966: O Toyota Corolla começou a ser produzido.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Abordagem baseada em *RAG***"
      ],
      "metadata": {
        "id": "3lpUooYIicQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***RAG*** (*Retrieval-Augmented Generation* - Geração aumentada de recuperação)"
      ],
      "metadata": {
        "id": "VryuJbWgqgFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outra ideia simples é dividir o texto em partes, mas em vez de extrair informações de cada parte, concentre-se apenas nas partes mais relevantes."
      ],
      "metadata": {
        "id": "lsrI6FvxmIFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Atenção**\n",
        "\n",
        "Pode ser difícil identificar quais partes são relevantes.\n",
        "\n",
        "Por exemplo, no artigo sobre carros que estamos usando aqui, a maior parte do artigo contém informações sobre desenvolvimentos chave. Então, ao usar ***RAG***, provavelmente estaremos descartando muitas informações relevantes.\n",
        "\n",
        "Sugerimos experimentar com seu caso de uso e determinar se essa abordagem funciona ou não.\n",
        "\n",
        "Para implementar a abordagem baseada em ***RAG***:\n",
        "\n",
        "1. Divida seu(s) documento(s) em partes e indexe-os (por exemplo, em um `vetorstore`);\n",
        "2. Adicione um passo de recuperação antes do extrator, utilizando o `vetorstore`."
      ],
      "metadata": {
        "id": "W52plXzTiCH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***RAG*** (*Retrieval-Augmented Generation*) é uma abordagem que combina recuperação de informações com geração de texto. A ideia principal é melhorar a capacidade de um modelo de linguagem ao integrar informações recuperadas de uma base de dados ou índice com a geração de texto."
      ],
      "metadata": {
        "id": "A6gHJpDfllgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como funciona o ***RAG***:\n",
        "\n",
        "   * **Recuperação**: quando um modelo precisa gerar uma resposta ou extrair informações, primeiro ele realiza uma busca em uma base de dados ou índice para recuperar partes relevantes do texto ou documentos que podem ajudar a responder a pergunta ou completar a tarefa. Esta base de dados é frequentemente um `vetorstore`, que armazena *embeddings* de texto para rápida recuperação.\n",
        "\n",
        "   * **Geração**: após recuperar as informações relevantes, o modelo de linguagem utiliza essas informações como contexto adicional para gerar uma resposta mais informada e precisa. Isso combina o conhecimento do modelo com dados específicos recuperados, aumentando a relevância e precisão da saída gerada."
      ],
      "metadata": {
        "id": "uNYY89Qflx-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***FAISS*** (*Facebook AI Similarity Search*) é uma biblioteca desenvolvida pelo Facebook AI Research para realizar busca eficiente e similaridade de alta dimensão em grandes conjuntos de dados. É projetada para lidar com grandes volumes de dados e facilitar a recuperação de informações baseadas em similaridade."
      ],
      "metadata": {
        "id": "XyeWNHnameDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***FAISS*** é especializado em encontrar vetores (representações numéricas de dados) similares a um vetor de consulta."
      ],
      "metadata": {
        "id": "nDV39Auimo6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Vectorstore*** é um termo usado para descrever um tipo de armazenamento especializado em vetores, frequentemente utilizado em sistemas de recuperação de informações e modelos de aprendizado de máquina. Ele é projetado para gerenciar e buscar vetores de alta dimensão de maneira eficiente."
      ],
      "metadata": {
        "id": "XKuF39_gm0sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU FAISS"
      ],
      "metadata": {
        "id": "TlvPTjpBm918"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo simples que utiliza o `vetorstore` `FAISS`."
      ],
      "metadata": {
        "id": "kVWrXJiGnc8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "textos = text_splitter.split_text(documento.page_content)\n",
        "vectorstore = FAISS.from_texts(textos, embedding = OpenAIEmbeddings())\n",
        "\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_kwargs={\"k\": 1}\n",
        ")  # extrai apenas do primeiro documento"
      ],
      "metadata": {
        "id": "kbwZ3KU8h_uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste caso, o extrator ***RAG*** está analisando apenas o principal documento."
      ],
      "metadata": {
        "id": "lr3WVRPnnk4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_extrator = {\n",
        "    \"texto\": retriever | (lambda docs: docs[0].page_content)\n",
        "} | extrator"
      ],
      "metadata": {
        "id": "caWVSiHnnm5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = rag_extrator.invoke(\"Principais desenvolvimentos associados aos carros\")"
      ],
      "metadata": {
        "id": "96CqLaHrn1SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for desenvolvimento in resposta.desenvolvimentos:\n",
        "    print(desenvolvimento)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd3xtBMooLex",
        "outputId": "814ee8c6-320c-46a4-b248-88181f82ac2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ano=2006 descricao='Crescimento de dois dígitos na receita e no número de membros de serviços de compartilhamento de carros nos EUA.' evidencia='...alguns serviços de compartilhamento de carros tiveram crescimento de dois dígitos na receita e no número de membros entre 2006 e 2007.'\n",
            "ano=2020 descricao='Produção de 56 milhões de carros em todo o mundo, uma queda em relação a 67 milhões no ano anterior.' evidencia='Em 2020, havia 56 milhões de carros fabricados em todo o mundo, down from 67 million the previous year.'\n",
            "ano=2020 descricao='A China produziu 20 milhões de carros, liderando a produção mundial.' evidencia='A indústria automotiva na China produz de longe o maior número (20 milhões em 2020)...'\n",
            "ano=2020 descricao='Cerca de um bilhão de carros estão nas estradas ao redor do mundo.' evidencia='Ao redor do mundo, há cerca de um bilhão de carros nas estradas.'\n",
            "ano=2019 descricao='Aumento da popularidade de sistemas de compartilhamento de bicicletas em cidades europeias e nos EUA.' evidencia='Sistemas de compartilhamento de bicicletas foram estabelecidos na China e em muitas cidades europeias, incluindo Copenhague e Amsterdã.'\n",
            "ano=2019 descricao='Estudo sobre os benefícios de introduzir bairros de baixo tráfego em Londres, mostrando que os benefícios superam os custos em 100 vezes nos primeiros 20 anos.' evidencia='Um estudo que verificou os custos e os benefícios de introduzir Low Traffic Neighbourhood em Londres encontrou que os benefícios superam os custos aproximadamente por 100 vezes nos primeiros 20 anos.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for desenvolvimento in sorted(resposta.desenvolvimentos, key=lambda x: x.ano):\n",
        "    print(f\"{desenvolvimento.ano}: {desenvolvimento.descricao}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VfP7lQFoCFb",
        "outputId": "b66f2170-4057-4ec9-838f-84b45ec8fd2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2006: Crescimento de dois dígitos na receita e no número de membros de serviços de compartilhamento de carros nos EUA.\n",
            "2019: Aumento da popularidade de sistemas de compartilhamento de bicicletas em cidades europeias e nos EUA.\n",
            "2019: Estudo sobre os benefícios de introduzir bairros de baixo tráfego em Londres, mostrando que os benefícios superam os custos em 100 vezes nos primeiros 20 anos.\n",
            "2020: Produção de 56 milhões de carros em todo o mundo, uma queda em relação a 67 milhões no ano anterior.\n",
            "2020: A China produziu 20 milhões de carros, liderando a produção mundial.\n",
            "2020: Cerca de um bilhão de carros estão nas estradas ao redor do mundo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE:**\n",
        "\n",
        "Métodos diferentes têm suas próprias vantagens e desvantagens relacionadas a custo, velocidade e precisão.\n",
        "\n",
        "Fique atento a esses problemas:\n",
        "\n",
        "- Dividir o conteúdo em partes significa que o LLM pode falhar em extrair informações se essas informações estiverem espalhadas por vários fragmentos.\n",
        "- Um grande sobreposição entre os fragmentos pode fazer com que a mesma informação seja extraída duas vezes, então esteja preparado para desduplicar!\n",
        "- LLMs podem inventar dados. Se você estiver procurando por um único fato em um texto longo e usar uma abordagem de força bruta, pode acabar obtendo mais dados inventados."
      ],
      "metadata": {
        "id": "Q9pm-gJzoyeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Como usar apenas o *prompting* para realizar a extração**"
      ],
      "metadata": {
        "id": "H6T0Amt3TRRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recursos de chamada de `ferramentas` não são necessários para gerar saídas estruturadas a partir de LLMs. LLMs que conseguem seguir bem as instruções do *prompt* podem ser encarregados de fornecer informações em um formato específico.\n",
        ">\n",
        "Essa abordagem depende de projetar bons *prompts* e, em seguida, fazer o *parser* da saída dos LLMs para garantir que eles extraíam informações de forma eficaz.\n",
        ">\n",
        "Para extrair dados sem recursos de chamada de `ferramentas`:\n",
        ">\n",
        "1. Instrua o LLM a gerar texto seguindo um formato esperado (por exemplo, `JSON` com um determinado esquema);\n",
        "2. Use *parsers* de saída para estruturar a resposta do modelo em um objeto Python desejado."
      ],
      "metadata": {
        "id": "U3ra5F8PU4cE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nem todos os modelos suportam `.with_structured_output()`, já que nem todos os modelos têm suporte para chamadas de `ferramentas` ou modo `JSON`. Para esses modelos, você precisará solicitar diretamente ao modelo que use um formato específico e usar um *parser* de saída para extrair a resposta estruturada da saída bruta do modelo.\n"
      ],
      "metadata": {
        "id": "JwiTIHxC6go0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Usando `PydanticOutputParser`**"
      ],
      "metadata": {
        "id": "EuhAtiWJ66MI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O exemplo a seguir usa o `PydanticOutputParser` embutido para fazer o *parser* da saída de um modelo de *chat* solicitado a corresponder ao esquema `Pydantic` fornecido. Note que estamos adicionando `format_instructions` diretamente ao *prompt* a partir de um método no *parser*:"
      ],
      "metadata": {
        "id": "shiUhUba6_xD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "class Pessoa(BaseModel):\n",
        "    \"\"\"Informação sobre uma pessoa.\"\"\"\n",
        "\n",
        "    nome: str = Field(..., description=\"O nome da pessoa.\")\n",
        "    altura_metros: float = Field(\n",
        "        ..., description=\"A altura da pessoa expressa em metros.\"\n",
        "    )\n",
        "\n",
        "class Pessoas(BaseModel):\n",
        "    \"\"\"Informações de identificação sobre todas as pessoas em um texto.\"\"\"\n",
        "\n",
        "    pessoas: List[Pessoa]\n",
        "\n",
        "# parser\n",
        "parser = PydanticOutputParser(pydantic_object=Pessoas)\n",
        "\n",
        "# prompt\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Responda à consulta do usuário. Envolva a saída em tags `json`\\n{format_instructions}\",\n",
        "        ),\n",
        "        (\"human\", \"{consulta}\"),\n",
        "    ]\n",
        ").partial(format_instructions = parser.get_format_instructions())"
      ],
      "metadata": {
        "id": "8iNncLbbTJB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos dar uma olhada em quais informações serão enviadas ao modelo:"
      ],
      "metadata": {
        "id": "OMLEIvFQ-LkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "consulta = \"Ana tem 23 anos e mede 1,83 metros de altura.\"\n",
        "\n",
        "print(prompt.invoke(consulta).to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw46s5HbTM23",
        "outputId": "d80640e9-aabc-42b8-8342-8e5f33ac1af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: Responda à consulta do usuário. Envolva a saída em tags `json`\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"description\": \"Informa\\u00e7\\u00f5es de identifica\\u00e7\\u00e3o sobre todas as pessoas em um texto.\", \"properties\": {\"pessoas\": {\"title\": \"Pessoas\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Pessoa\"}}}, \"required\": [\"pessoas\"], \"definitions\": {\"Pessoa\": {\"title\": \"Pessoa\", \"description\": \"Informa\\u00e7\\u00e3o sobre uma pessoa.\", \"type\": \"object\", \"properties\": {\"nome\": {\"title\": \"Nome\", \"description\": \"O nome da pessoa.\", \"type\": \"string\"}, \"altura_metros\": {\"title\": \"Altura Metros\", \"description\": \"A altura da pessoa expressa em metros.\", \"type\": \"number\"}}, \"required\": [\"nome\", \"altura_metros\"]}}}\n",
            "```\n",
            "Human: Ana tem 23 anos e mede 1,83 metros de altura.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "E agora, vamos invocá-lo:"
      ],
      "metadata": {
        "id": "hB8IhwrJ-ZlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | modelo | parser\n",
        "\n",
        "resposta = chain.invoke({\"consulta\": consulta})"
      ],
      "metadata": {
        "id": "CEsllJoPTV1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAUEHVGS9o49",
        "outputId": "b40997e7-8a77-410f-f557-e39d88e8750a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pessoas(pessoas=[Pessoa(nome='Ana', altura_metros=1.83)])"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "_JlQcUbfWQ0T",
        "outputId": "7f634b11-d6b1-4163-8f7e-25b6939e8bc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.Pessoas"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>Pessoas</b><br/>def __init__(__pydantic_self__, **data: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\"></a>Informações de identificação sobre todas as pessoas em um texto.</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta.pessoas[0].nome"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "prAhp-AY9h6t",
        "outputId": "3e2b9463-a6f9-4662-d3b7-5d8bacb850ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ana'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for pessoa in resposta.pessoas:\n",
        "    print(pessoa.nome)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60oG2oihWeu_",
        "outputId": "afabb29c-0650-49a5-ac26-860548ec3a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ana\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Parser* Personalizado**"
      ],
      "metadata": {
        "id": "yKzFyD-j-mg5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Você também pode criar um *prompt* e um *parser* personalizados com a **Linguagem de Expressão LangChain** (`LCEL`), usando uma função simples para fazer o *parser* da saída do modelo:"
      ],
      "metadata": {
        "id": "1UAHjMqp-mnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "from langchain_core.messages import AIMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "class Pessoa(BaseModel):\n",
        "    \"\"\"Informação sobre uma pessoa.\"\"\"\n",
        "\n",
        "    nome: str = Field(..., description=\"O nome da pessoa.\")\n",
        "    altura_metros: float = Field(\n",
        "        ..., description=\"A altura da pessoa expressa em metros.\"\n",
        "    )\n",
        "\n",
        "class Pessoas(BaseModel):\n",
        "    \"\"\"Informações de identificação sobre todas as pessoas em um texto.\"\"\"\n",
        "\n",
        "    pessoas: List[Pessoa]\n",
        "\n",
        "# prompt\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Responda à consulta do usuário. Saia com sua resposta como JSON que  \"\n",
        "            \"corresponda ao esquema fornecido: json\\n{schema}\\n. \"\n",
        "            \"Certifique-se de envolver a resposta em tags json e .\",\n",
        "        ),\n",
        "        (\"human\", \"{consulta}\"),\n",
        "    ]\n",
        ").partial(schema=Pessoas.schema())\n",
        "\n",
        "\n",
        "# parser personalizado\n",
        "def extrair_json(mensagem: AIMessage) -> List[dict]:\n",
        "    \"\"\"Extraia o conteúdo JSON de uma string em que o JSON está embutido entre as tags ```json e ```.\n",
        "\n",
        "    Parâmetros:\n",
        "        texto (str): O texto contendo o conteúdo JSON.\n",
        "\n",
        "    Retorna:\n",
        "        lista: Uma lista de strings JSON extraídas.\n",
        "    \"\"\"\n",
        "    texto = mensagem.content\n",
        "\n",
        "    # padrão da expressão regular para corresponder a blocos JSON\n",
        "    padrao = r\"```json(.*?)```\"\n",
        "\n",
        "    # correspondências não sobrepostas do padrão na string\n",
        "    matches = re.findall(padrao, texto, re.DOTALL)\n",
        "\n",
        "    # retorne a lista de strings JSON correspondentes, removendo quaisquer espaços em branco no início ou no final\n",
        "    try:\n",
        "        return [json.loads(match.strip()) for match in matches]\n",
        "    except Exception:\n",
        "        raise ValueError(f\"Falha no parser: {mensagem}\")"
      ],
      "metadata": {
        "id": "0XoG6qcxTf-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"Ana tem 23 anos e mede 1,83 metros de altura.\"\n",
        "\n",
        "print(prompt.format_prompt(consulta = texto).to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uXHH15DTjsM",
        "outputId": "26195622-37f7-4026-8d64-dd0111c7c320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: Responda à consulta do usuário. Saia com sua resposta como JSON que  corresponda ao esquema fornecido: json\n",
            "{'title': 'Pessoas', 'description': 'Informações de identificação sobre todas as pessoas em um texto.', 'type': 'object', 'properties': {'pessoas': {'title': 'Pessoas', 'type': 'array', 'items': {'$ref': '#/definitions/Pessoa'}}}, 'required': ['pessoas'], 'definitions': {'Pessoa': {'title': 'Pessoa', 'description': 'Informação sobre uma pessoa.', 'type': 'object', 'properties': {'nome': {'title': 'Nome', 'description': 'O nome da pessoa.', 'type': 'string'}, 'altura_metros': {'title': 'Altura Metros', 'description': 'A altura da pessoa expressa em metros.', 'type': 'number'}}, 'required': ['nome', 'altura_metros']}}}\n",
            ". Certifique-se de envolver a resposta em tags json e .\n",
            "Human: Ana tem 23 anos e mede 1,83 metros de altura.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | modelo | extrair_json\n",
        "\n",
        "resposta = chain.invoke({\"consulta\": texto})"
      ],
      "metadata": {
        "id": "pl-BoF91TmyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvhJGwmeB4ar",
        "outputId": "224c4006-38d3-4f9a-b732-8a7ad8608988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'pessoas': [{'nome': 'Ana', 'altura_metros': 1.83}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8DchUNmCpTh",
        "outputId": "8e6a757d-3f5a-4739-c1e8-89d19ea130cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta[0]['pessoas'][0]['nome']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "G9YqE9piB6NN",
        "outputId": "47003715-0ec3-4397-ea3a-ae7ca8ec5158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ana'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Referências:**"
      ],
      "metadata": {
        "id": "oe14pd2FVNcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> https://python.langchain.com/v0.2/docs/tutorials/extraction/\n",
        "\n",
        "> https://python.langchain.com/v0.2/docs/how_to/extraction_long_text/\n",
        "\n",
        "> https://python.langchain.com/v0.2/docs/how_to/extraction_parse/"
      ],
      "metadata": {
        "id": "FgdLNhmITBCi"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-lpmLOvr7s7G",
        "H6T0Amt3TRRK",
        "oe14pd2FVNcC"
      ],
      "authorship_tag": "ABX9TyN4R5xxo7YsANlKBdYUmuyd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}