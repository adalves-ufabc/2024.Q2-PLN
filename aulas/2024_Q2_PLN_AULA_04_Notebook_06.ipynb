{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VJmUHTmjtaNa",
        "XirjLq-9tcqJ"
      ],
      "authorship_tag": "ABX9TyOZQxIqx8u4BYMuoWDxQmWC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalves-ufabc/2024.Q2-PLN/blob/main/2024_Q2_PLN_AULA_04_Notebook_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2024-Q2]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Correção Ortogográfica**\n",
        "---"
      ],
      "metadata": {
        "id": "cXNV4odoXftj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correção ortográfica em PLN é importante por vários motivos:\n",
        "\n",
        "  * Melhorar a precisão da análise de texto: a correção ortográfica ajuda a evitar erros de interpretação causados por palavras escritas incorretamente. Isso é especialmente importante para tarefas de análise de texto, como reconhecimento de entidades nomeadas, análise de sentimento e classificação de texto.\n",
        "\n",
        "  * Aumentar a eficiência da análise: a correção ortográfica permite que os algoritmos de PLN processem o texto com mais eficiência, reduzindo o número de variações de palavras que precisam ser consideradas.\n",
        "\n",
        "  * Melhorar a experiência do usuário: corrigir erros de ortografia em texto pode melhorar a compreensão do leitor e a credibilidade do autor. Isso é especialmente importante em aplicações de PLN voltadas para usuários finais, como assistentes virtuais e aplicativos de mensagens.\n",
        "\n",
        "  * Padronizar os dados de texto: a correção ortográfica pode ajudar a padronizar a ortografia de palavras em um conjunto de dados de texto, tornando-o mais fácil de analisar e comparar.\n"
      ],
      "metadata": {
        "id": "p4hlX1pbYs-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo de correção ortográfica em português usando a biblioteca `pyspellchecker` em `Python`:"
      ],
      "metadata": {
        "id": "pRPrtVYUXupO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GT4dPlLhXDox",
        "outputId": "186cd668-3094-42c9-eb18-a777a380bab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "texto = \"Eu gosto de caminhar pela praja\"\n",
        "\n",
        "corretor = SpellChecker(language='pt')\n",
        "palavras = texto.split()\n",
        "\n",
        "palavras_corrigidas = []\n",
        "for palavra in palavras:\n",
        "    palavra_corrigida = corretor.correction(palavra)\n",
        "    palavras_corrigidas.append(palavra_corrigida)"
      ],
      "metadata": {
        "id": "HS3hc3AmXAop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(palavras_corrigidas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLpAGC16Lt0y",
        "outputId": "023a75dd-4320-4f96-a11e-e2fc408e357e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Eu', 'gosto', 'de', 'caminhar', 'pela', 'praia']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texto_corrigido = \" \".join(palavras_corrigidas)\n",
        "\n",
        "print(texto_corrigido)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEChCb55Lsxi",
        "outputId": "414f8074-5c04-49dc-9af7-899545b5e611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eu gosto de caminhar pela praia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesse exemplo, foi definida uma frase com uma palavra escrita incorretamente (`praja` em vez de `praia`). Em seguida, a biblioteca `pyspellchecker` é usada para corrigir a ortografia da palavra.\n",
        "\n",
        "Para isso, criamos um objeto `SpellChecker` e o usamos para verificar a ortografia de cada palavra na frase usando o método `correction()`. Em seguida, adicionamos as palavras corrigidas a uma nova lista e, por fim, unimos as palavras em uma nova frase corrigida usando a função `join`."
      ],
      "metadata": {
        "id": "noOiTUXNX55O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo simples de correção ortográfica em `Python` usando a biblioteca `autocorrect`:"
      ],
      "metadata": {
        "id": "vDoo3YsDZ156"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instalar a biblioteca autocorrect\n",
        "!pip install autocorrect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efTvA0gtZR4D",
        "outputId": "dce6dca4-6f70-4c97-96f8-eaf2cd7bfce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622363 sha256=8a7d3b6d9ac41fefec87dac607abeca52a6ab2c66c6109e00a4ada26eb03ac01\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/7b/6d/b76b29ce11ff8e2521c8c7dd0e5bfee4fb1789d76193124343\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autocorrect import Speller\n",
        "\n",
        "# criar um objeto Speller com o idioma \"Portuguese\"\n",
        "corrector = Speller(lang='pt')\n",
        "\n",
        "# texto com um erro de ortografia\n",
        "texto = \"Este é um exempel de frasi com errro de ortografia.\"\n",
        "\n",
        "# corrigir o erro de ortografia no texto\n",
        "texto_corrigido = corrector(texto)\n",
        "\n",
        "# imprimir o texto corrigido\n",
        "print(texto_corrigido)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8Nt6RvDZPeV",
        "outputId": "8d28544d-aba0-4402-8c90-80ed5c8037b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Este é um exemplo de frase com erro de ortografia.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Capitalização**\n",
        "---"
      ],
      "metadata": {
        "id": "VJmUHTmjtaNa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDD7hanr_SMK"
      },
      "source": [
        "Processo de colocar os tokens em letra minúscula para normalização do texto."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4jkS59hrZ8W",
        "outputId": "c162c5bb-a082-4f93-b910-4486fab4e328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYp6P1WX_Vhw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7e734b5-0a83-4730-831d-95225bcd469b"
      },
      "source": [
        "versos = \"\"\"No meio do caminho tinha uma pedra\n",
        "Tinha uma pedra no meio do caminho\"\"\".lower()\n",
        "\n",
        "palavras = nltk.word_tokenize(versos, language='portuguese')\n",
        "\n",
        "print(len(palavras), palavras, '\\n', len(set(palavras)), set(palavras))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14 ['no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', 'tinha', 'uma', 'pedra', 'no', 'meio', 'do', 'caminho'] \n",
            " 7 {'no', 'meio', 'tinha', 'do', 'caminho', 'uma', 'pedra'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Segmentação de Sentenças**\n",
        "---"
      ],
      "metadata": {
        "id": "XirjLq-9tcqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk.download('punkt')\n",
        "\n",
        "texto = \"\"\"Eu estou na aula de Processamento de Linguagem Natural.\n",
        "O exemplo é muito simples.\"\"\"\n",
        "\n",
        "nltk.sent_tokenize(texto, language='portuguese')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJBRLIbetf5F",
        "outputId": "6e9381ed-d81d-446c-bfdc-df813676e1cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Eu estou na aula de Processamento de Linguagem Natural.',\n",
              " 'O exemplo é muito simples.']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Radicalização (Stemização)**\n",
        "---"
      ],
      "metadata": {
        "id": "IRDS3RxoapDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **stemização** (*stemming*) é um processo de normalização de palavras em linguística computacional e processamento de linguagem natural. O objetivo da **stemização** é reduzir as palavras em sua forma raiz ou *stem*, de modo que palavras semelhantes possam ser agrupadas juntas. Isso é útil em aplicações como análise de sentimentos, recuperação de informações e classificação de documentos."
      ],
      "metadata": {
        "id": "TUT1d7_3kDed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O processo de **stemização** envolve a remoção de sufixos e prefixos das palavras, deixando apenas o núcleo ou raiz da palavra. Por exemplo, a palavra `corrida` pode ser reduzida ao seu *stem* `corrid`, que pode ser usado para agrupar palavras relacionadas, como `correr`, `corredor` e `corredora`."
      ],
      "metadata": {
        "id": "FQCnDpuLNOiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Existem diferentes algoritmos e técnicas para realizar a **stemização**, como o algoritmo de `Porter`, o `Snowball` e o algoritmo de `Lancaster`. Cada um desses algoritmos tem suas próprias regras e heurísticas para reduzir as palavras à sua forma raiz. A escolha do algoritmo depende da língua em que o processamento será feito e da aplicação específica em questão."
      ],
      "metadata": {
        "id": "1nMCXElsNOkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo de código em `Python` usando a biblioteca `NLTK` para realizar a stemização de palavras em português:"
      ],
      "metadata": {
        "id": "TnMBuq5TlS49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPT4nugStArn",
        "outputId": "10128102-ffd6-493a-b2f0-aa0365c8af2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(\"portuguese\")\n",
        "\n",
        "def stem_texto(texto):\n",
        "    words = nltk.word_tokenize(texto, language='portuguese')\n",
        "    stems = [stemmer.stem(word) for word in words]\n",
        "    stemmed_text = \" \".join(stems)\n",
        "    return stemmed_text"
      ],
      "metadata": {
        "id": "C2urXssXlMRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"Estou correndo no parque e vi um corredor correndo mais rápido do que eu.\"\n",
        "texto_stemmed = stem_texto(texto)\n",
        "\n",
        "print(texto_stemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxTOblOcN3bw",
        "outputId": "614e2473-ad07-4a01-9623-823fa90c11bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "estou corr no parqu e vi um corredor corr mais ráp do que eu .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está outro exemplo em `Python` usando a biblioteca `NLTK` para realizar a stemização em português:"
      ],
      "metadata": {
        "id": "N9PwonqMmt0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('rslp')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ekmTbgHmg1x",
        "outputId": "14da3f1c-9501-4aed-cc86-12656703ab2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "raiz = nltk.stem.RSLPStemmer()\n",
        "\n",
        "tokens = nltk.word_tokenize('A comida estava gostosa', language='portuguese')\n",
        "[raiz.stem(token) for token in tokens]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYlfq1C807JE",
        "outputId": "0c8b32cb-c924-4c15-b40b-7b4abaa21d3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'com', 'est', 'gost']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mais um exemplo usando RSLP:"
      ],
      "metadata": {
        "id": "z7zkN72T0-6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RSLPStemmer\n",
        "\n",
        "stemmer = RSLPStemmer()\n",
        "\n",
        "def stem_texto(texto):\n",
        "    words = nltk.word_tokenize(texto)\n",
        "    stems = [stemmer.stem(word) for word in words]\n",
        "    stemmed_text = \" \".join(stems)\n",
        "    return stemmed_text"
      ],
      "metadata": {
        "id": "qtI86nlmmeEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"Eu estou correndo no parque e vi um corredor correndo mais rápido do que eu.\"\n",
        "texto_stemmed = stem_texto(texto)\n",
        "\n",
        "print(texto_stemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNovvTzqOH3C",
        "outputId": "752fd032-b7d3-4da1-8010-45de9961d917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eu est corr no parqu e vi um corr corr mais rápid do que eu .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe que a stemização não é perfeita e, às vezes, pode levar a resultados imprecisos. Além disso, a stemização é uma forma de pré-processamento de texto e, portanto, deve ser aplicada antes de outras etapas.\n"
      ],
      "metadata": {
        "id": "D4yHflh6nADK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo de código em `Python` usando a biblioteca `NLTK` para realizar a stemização de palavras em inglês:"
      ],
      "metadata": {
        "id": "2-ySihr3lcVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_text(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    stems = [stemmer.stem(word) for word in words]\n",
        "    stemmed_text = \" \".join(stems)\n",
        "    return stemmed_text"
      ],
      "metadata": {
        "id": "sxfFKM4KlYER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am running in the park and saw a runner running faster than me.\"\n",
        "stemmed_text = stem_text(text)\n",
        "\n",
        "print(stemmed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoxZhvDfO0ng",
        "outputId": "cf411cd4-5f89-4785-8c88-c3618c2c052f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i am run in the park and saw a runner run faster than me .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está outro exemplo:"
      ],
      "metadata": {
        "id": "Y32Pry14rMIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "tokens = ['compute', 'computer', 'computed', 'computing']\n",
        "\n",
        "for token in tokens:\n",
        "    print(token + ' --> ' + stemmer.stem(token))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjVqJKVZrJGA",
        "outputId": "fc905abc-7265-41bc-ed17-aa10f189ab6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compute --> comput\n",
            "computer --> comput\n",
            "computed --> comput\n",
            "computing --> comput\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lematização**\n",
        "---"
      ],
      "metadata": {
        "id": "LK4dHF4JawmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **lematização** (*lemmatization*) é um processo de normalização de palavras em linguística computacional e processamento de linguagem natural. O objetivo da **lematização** é reduzir palavras em sua forma canônica ou base, conhecida como *lemma*, de modo que palavras com significados semelhantes possam ser agrupadas juntas. Isso é útil em aplicações como análise de sentimentos, recuperação de informações e classificação de documentos.\n"
      ],
      "metadata": {
        "id": "tkLsonzskbwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ao contrário da stemização, a **lematização** não apenas remove os sufixos e prefixos das palavras, mas também leva em consideração a classe gramatical da palavra e sua flexão. Por exemplo, a palavra `correndo` pode ser reduzida ao seu *lemma* `correr`, enquanto a palavra `corrida` pode ser reduzida ao seu *lemma* `corrida`.\n",
        "\n"
      ],
      "metadata": {
        "id": "xThyi9E9PEdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Existem diferentes algoritmos e técnicas para realizar a **lematização**, como o algoritmo `WordNet` e o algoritmo de **lematização** baseado em regras. Cada algoritmo tem suas próprias regras e heurísticas para reduzir as palavras ao seu lemma. A escolha do algoritmo também depende da língua em que o processamento será feito e da aplicação específica em questão."
      ],
      "metadata": {
        "id": "rmjytwzbPGfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo de código em `Python` usando a biblioteca `NLTK` para realizar a lematização:"
      ],
      "metadata": {
        "id": "Y41CLp9orgk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# baixar o corpus do WordNet, se ainda nao foi baixado\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "742czm8BrnJF",
        "outputId": "da398791-a9ca-43cb-ce74-6ab661e8f67f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código `nltk.download('wordnet')` faz o download do córpus do `WordNet`, que é um dicionário léxico online usado para a lematização, a consulta de sinônimos e outras tarefas relacionadas ao PLN."
      ],
      "metadata": {
        "id": "HQO7Okh9r4pF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código `nltk.download('omw-1.4')` faz o download do córpus do *Open Multilingual WordNet* (OMW), que é um recurso linguístico que oferece sinônimos em vários idiomas. O `OMW` é baseado no `WordNet`, mas em vez de se concentrar exclusivamente no inglês, ele inclui sinônimos em várias línguas, como francês, italiano, espanhol e outros."
      ],
      "metadata": {
        "id": "YpGiymtvsJEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "text = \"Os gatos estão correndo e brincando juntos\"\n",
        "\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "lemmas = []\n",
        "for token in tokens:\n",
        "    lemma = lemmatizer.lemmatize(token)\n",
        "    lemmas.append(lemma)"
      ],
      "metadata": {
        "id": "K48DRnqariYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wz7WxfVtQwcX",
        "outputId": "e998009f-ff6f-42c4-9d07-62a0716d6605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Os', 'gatos', 'estão', 'correndo', 'e', 'brincando', 'junto']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo de como realizar a lematização usando a biblioteca `spaCy` em `Python`:"
      ],
      "metadata": {
        "id": "lsS7Btof6Nbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download pt_core_news_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fGFVTomxUX3",
        "outputId": "bd7f5a4a-b93b-425a-f4c0-7118e0d26a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pt-core-news-lg==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.7.0/pt_core_news_lg-3.7.0-py3-none-any.whl (568.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.2/568.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from pt-core-news-lg==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.7.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.1.2)\n",
            "Installing collected packages: pt-core-news-lg\n",
            "Successfully installed pt-core-news-lg-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# carregar o modelo de linguagem em portugues\n",
        "nlp = spacy.load('pt_core_news_lg')\n",
        "\n",
        "# texto de exemplo\n",
        "texto = \"Eu estava correndo no parque quando vi um bando de pássaros.\"\n",
        "\n",
        "# processar o texto com o modelo de linguagem\n",
        "doc = nlp(texto)"
      ],
      "metadata": {
        "id": "cSP0SwyG6BpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imprimir as palavras e seus lemas\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9BnkljlRVx_",
        "outputId": "137ac0d7-6730-40c4-9e56-4f8171546adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eu eu\n",
            "estava estar\n",
            "correndo correr\n",
            "no em o\n",
            "parque parque\n",
            "quando quando\n",
            "vi ver\n",
            "um um\n",
            "bando bando\n",
            "de de\n",
            "pássaros pássaro\n",
            ". .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe que as palavras `estava` e `correndo` foram reduzidas ao seu lema `estar` e `correr`, respectivamente."
      ],
      "metadata": {
        "id": "RvL8fq6f6rUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo de código para a lematização e stemização de palavras usando a biblioteca `NLTK` em Python:"
      ],
      "metadata": {
        "id": "3Xi9KXYE7oSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "\n",
        "# instanciar um objeto lematizador do WordNet\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# instanciar um objeto stemizador para o idioma português\n",
        "stemmer = SnowballStemmer(\"portuguese\")\n",
        "\n",
        "# texto de exemplo\n",
        "texto = \"Os cachorros correm no parque enquanto os pássaros cantam.\"\n",
        "\n",
        "# realizar a tokenizacao do texto\n",
        "tokens = nltk.word_tokenize(texto)\n",
        "\n",
        "# inicializar listas para armazenar as palavras lematizadas e stemizadas\n",
        "lemas = []\n",
        "stems = []\n",
        "\n",
        "# percorrer cada token e realizar a lematização e a stemização\n",
        "for token in tokens:\n",
        "    # lematizacao\n",
        "    lemma = lemmatizer.lemmatize(token)\n",
        "    lemas.append(lemma)\n",
        "\n",
        "    # stemizacao\n",
        "    stem = stemmer.stem(token)\n",
        "    stems.append(stem)"
      ],
      "metadata": {
        "id": "efAZzc3v7jSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imprimir o texto lematizado e stemizado\n",
        "print(\"Texto original: \", texto)\n",
        "print(\"Lematizado: \", \" \".join(lemas))\n",
        "print(\"Stemizado: \", \" \".join(stems))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VWu2paLStg4",
        "outputId": "5d5ba7ce-0048-41a3-8818-d11f5b3ce94b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original:  Os cachorros correm no parque enquanto os pássaros cantam.\n",
            "Lematizado:  Os cachorros correm no parque enquanto o pássaros cantam .\n",
            "Stemizado:  os cachorr corr no parqu enquant os pássar cant .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh4gud-2yK5Q",
        "outputId": "e2c40e75-6a80-43ca-cf2d-1cd4dce7ec71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "from collections import Counter\n",
        "\n",
        "# instanciar um objeto lematizador do WordNet\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# instanciar um objeto stemizador para o idioma ingles\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# definir o texto de exemplo\n",
        "texto = \"The quick brown fox jumped over the lazy dog. The dog was not amused.\"\n",
        "\n",
        "# realizar a tokenização do texto e remover as stopwords\n",
        "tokens = [token.lower() for token in word_tokenize(texto) if token.isalpha() and token.lower() not in stopwords.words(\"english\")]\n",
        "\n",
        "# inicializa listas para armazenar as palavras lematizadas e stemizadas\n",
        "lemas = []\n",
        "stems = []\n",
        "\n",
        "# percorrer cada token e realizar a lematizacao e a stemizacao\n",
        "for token in tokens:\n",
        "    # lematizacao\n",
        "    lemma = lemmatizer.lemmatize(token)\n",
        "    lemas.append(lemma)\n",
        "\n",
        "    # stemizacao\n",
        "    stem = stemmer.stem(token)\n",
        "    stems.append(stem)\n",
        "\n",
        "# criar um contador para contar a frequencia de cada palavra\n",
        "frequencias = Counter(tokens)"
      ],
      "metadata": {
        "id": "SONf_cO074uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imprimir os resultados\n",
        "print(\"Texto original: \", texto)\n",
        "print(\"Tokens: \", tokens)\n",
        "print(\"Palavras unicas: \", set(tokens))\n",
        "print(\"Frequencias: \", frequencias)\n",
        "print(\"Lemas: \", lemas)\n",
        "print(\"Stems: \", stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCx7hoZNTPZW",
        "outputId": "f319488c-ff83-4910-f964-2b6e8014b486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original:  The quick brown fox jumped over the lazy dog. The dog was not amused.\n",
            "Tokens:  ['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog', 'dog', 'amused']\n",
            "Palavras unicas:  {'brown', 'amused', 'dog', 'lazy', 'fox', 'jumped', 'quick'}\n",
            "Frequencias:  Counter({'dog': 2, 'quick': 1, 'brown': 1, 'fox': 1, 'jumped': 1, 'lazy': 1, 'amused': 1})\n",
            "Lemas:  ['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog', 'dog', 'amused']\n",
            "Stems:  ['quick', 'brown', 'fox', 'jump', 'lazi', 'dog', 'dog', 'amus']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe que a função remove as *stopwords* (palavras comuns que não agregam muito significado ao texto, como `the` e `was`) antes de realizar a lematização e a stemização. Além disso, o código utiliza um objeto `Counter` do `Python` para contar a frequência de cada palavra no texto."
      ],
      "metadata": {
        "id": "DrCI4xx38GqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Normalização de Textos**\n",
        "---"
      ],
      "metadata": {
        "id": "Pb-grWGcBsBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalização de textos** é o processo de transformar textos em uma forma mais consistente e uniforme. O objetivo da normalização é tornar o texto mais fácil de ser processado por algoritmos, melhorar a qualidade da análise de texto e tornar as comparações de palavras e frases mais precisas."
      ],
      "metadata": {
        "id": "n-0b48jvES78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **normalização** pode incluir a remoção de caracteres especiais, a correção ortográfica, a padronização de maiúsculas e minúsculas, a remoção de pontuação, a remoção de *stopwords*  e a lematização/stemização.\n",
        "\n"
      ],
      "metadata": {
        "id": "-S5qOqTzTsw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **normalização de texto** é uma tarefa importante em várias aplicações de PLN, incluindo análise de sentimento, classificação de texto, tradução automática, reconhecimento de fala etc."
      ],
      "metadata": {
        "id": "g7-o6uxJT6WJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo de normalização de texto que combina o uso do módulo `unicodedata` do `Python` com expressões regulares:"
      ],
      "metadata": {
        "id": "CbODIN9dCD54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "# definir um texto com caracteres acentuados, espaços extras e caracteres especiais\n",
        "texto = \"    Olá! Tudo bem? Eu gostaria de pedir um café expresso, por favor.   ☕️ \"\n",
        "\n",
        "# remover caracteres especiais e acentos utilizando o modulo unicodedata\n",
        "texto_sem_acentos = unicodedata.normalize('NFKD', texto).encode('ASCII', 'ignore').decode('ASCII')\n",
        "\n",
        "# remover espacos extras utilizando expressoes regulares\n",
        "texto_normalizado = re.sub(r'\\s+', ' ', texto_sem_acentos).strip()\n",
        "\n",
        "# imprimir o texto normalizado\n",
        "print(texto_normalizado)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIdGL9JBBZwb",
        "outputId": "0a4a7b0c-73d6-436c-e3ec-6e51fdc278c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ola! Tudo bem? Eu gostaria de pedir um cafe expresso, por favor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esse processo de normalização pode ser útil para padronizar o texto e reduzir a variação de caracteres em diferentes fontes de dados. Ele também pode ajudar a simplificar a análise de texto, especialmente quando há diferenças de codificação entre as fontes de dados."
      ],
      "metadata": {
        "id": "HKKoSC3dCN9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo de normalização de texto que utiliza expressões regulares para substituir padrões específicos de caracteres:"
      ],
      "metadata": {
        "id": "GUbNIGbeCVKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# definir um texto com abreviacoes e girias\n",
        "texto = \"Eu n sei se vc vai cmg, mas acho q a galera vai. Vou t mandar um zap dps\"\n",
        "\n",
        "# definir um dicionario de substituicao\n",
        "substituicoes = {\n",
        "    r'n sei': 'não sei',\n",
        "    r'vc': 'você',\n",
        "    r'cmg': 'comigo',\n",
        "    r'q': 'que',\n",
        "    r'galera': 'turma',\n",
        "    r't ': 'te ',\n",
        "    r'zap': 'mensagem',\n",
        "    r'dps': 'depois'\n",
        "}\n",
        "\n",
        "# iterar sobre as chaves do dicionario e aplicar as substituicoes com expressoes regulares\n",
        "for padrao, substituicao in substituicoes.items():\n",
        "    texto = re.sub(padrao, substituicao, texto)\n",
        "\n",
        "# imprimir o resultado\n",
        "print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQQ_pWM0Bxzp",
        "outputId": "ac13272c-4a11-493d-c3f1-c4f44b9a54ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eu não sei se você vai comigo, mas acho que a turma vai. Vou te mandar um mensagem depois\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesse exemplo, expressões regulares são usadas para substituir abreviações e gírias por palavras completas. O dicionário `substituicoes` contém as chaves que representam os padrões de caracteres que serão substituídos e os valores que representam as palavras completas que serão utilizadas como substituição. Em seguida, iteramos sobre as chaves do dicionário e aplicamos as substituições com a função `re.sub()` da biblioteca `re`. O resultado é um texto normalizado que utiliza palavras completas em vez de abreviações e gírias."
      ],
      "metadata": {
        "id": "MzPxKyUgCbOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está outro exemplo de normalização de texto em `Python`, usando a biblioteca `re` e a técnica de remoção de caracteres especiais:"
      ],
      "metadata": {
        "id": "1MpRQ8x0HVrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# definir uma string com caracteres especiais\n",
        "texto = \"Boa noite!!! 😃🎉🔥🎂👋🏼👨🏻‍💻\"\n",
        "\n",
        "# remover caracteres especiais\n",
        "texto_sem_caracteres_especiais = re.sub(r'[^\\w\\s]', '', texto)\n",
        "\n",
        "# imprimir o texto normalizado\n",
        "print(texto_sem_caracteres_especiais)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jv9JfuxCHRpA",
        "outputId": "52ce4aa8-9185-4b15-acbe-93e1941b61b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Boa noite \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# definir um tweet para normalizar\n",
        "tweet = \"Acabei de assistir um filme incrível! @Netflix, vocês arrasaram! 🎥🍿 http://netflix.com/filme\"\n",
        "\n",
        "# remover URLs e mencoes\n",
        "tweet_sem_url = re.sub(r\"http\\S+\", \"\", tweet) # remover URLs\n",
        "tweet_sem_mencao = re.sub(r\"@\\S+\", \"\", tweet_sem_url) # remover menções\n",
        "\n",
        "# imprimir o tweet normalizado\n",
        "print(tweet_sem_mencao)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4s7B64bIYHk",
        "outputId": "f72ea826-cbb4-4644-f82e-b68898678d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acabei de assistir um filme incrível!  vocês arrasaram! 🎥🍿 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# definir um texto para normalizar\n",
        "texto = \"Esta frase tem! muita pontuação, certo? Mas vamos normalizar.\"\n",
        "\n",
        "# transformar todas as letras em minusculas\n",
        "texto = texto.lower()\n",
        "\n",
        "# tokenizar o texto e remover a pontuacao\n",
        "tokens = texto.split()\n",
        "tokens_sem_pontuacao = [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens]\n",
        "\n",
        "# imprimir os tokens normalizados\n",
        "print(tokens_sem_pontuacao)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqNnqLgBIpel",
        "outputId": "d88aa2a6-acac-4ff9-8da5-cfe06b809ddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['esta', 'frase', 'tem', 'muita', 'pontuação', 'certo', 'mas', 'vamos', 'normalizar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# definir um texto em portugues com uma data\n",
        "texto = \"A reunião está marcada para o dia 15/03/2022.\"\n",
        "\n",
        "# normalizar a data para o formato 'YYYY-MM-DD'\n",
        "padrao_data = r'(\\d{2})/(\\d{2})/(\\d{4})'\n",
        "texto_normalizado = re.sub(padrao_data, r'\\3-\\2-\\1', texto)\n",
        "\n",
        "# imprimir o texto normalizado\n",
        "print(texto_normalizado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDSQWPNiJkDH",
        "outputId": "a835cf4e-8299-4fbb-fb07-29d1a032b6d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A reunião está marcada para o dia 2022-03-15.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# definir um texto em portugues com um numero de telefone\n",
        "texto = \"Meu número de telefone é (11) 99999-7777.\"\n",
        "\n",
        "# normalizar o numero de telefone para o formato '11999999999'\n",
        "padrao_telefone = r'\\((\\d{2})\\)\\s+(\\d{5})-(\\d{4})'\n",
        "texto_normalizado = re.sub(padrao_telefone, r'\\1\\2\\3', texto)\n",
        "\n",
        "# imprimir o texto normalizado\n",
        "print(texto_normalizado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBVhgPZlJ78H",
        "outputId": "67638102-346a-4a75-cb67-89041c6cd0bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu número de telefone é 11999997777.\n"
          ]
        }
      ]
    }
  ]
}